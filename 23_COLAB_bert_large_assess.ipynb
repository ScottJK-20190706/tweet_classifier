{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"25_COLAB_bert_large_assess.ipynb","provenance":[{"file_id":"1EeAvFrQR4rDEYuAbt1xZRlYoku_OH-08","timestamp":1572701398470},{"file_id":"1qecLli5EEPPufgRsER6tGDUlpHbvB_GG","timestamp":1572689385842}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xcoYWhZ41-Qa","colab_type":"text"},"source":["## Install Tensorflow"]},{"cell_type":"code","metadata":{"id":"406MmBYXZXmZ","colab_type":"code","outputId":"cb6c1567-cfd3-4051-c9ec-9dd3f2209122","executionInfo":{"status":"ok","timestamp":1572711172215,"user_tz":0,"elapsed":3758,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=ynDmeatCWLJK\n","!pip install bert-tensorflow"],"execution_count":103,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KgDzFHAl2LFo","colab_type":"text"},"source":["## Check that TPU is being used"]},{"cell_type":"code","metadata":{"id":"_BxipnlyQs0G","colab_type":"code","outputId":"41ffdac5-5136-40a8-ec32-f139c45ed9db","executionInfo":{"status":"ok","timestamp":1572711176739,"user_tz":0,"elapsed":1546,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["import datetime\n","import json\n","import os\n","import pprint\n","import random\n","import string\n","import sys\n","import tensorflow as tf\n","import bert\n","import numpy as np\n","\n","\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","with tf.Session(TPU_ADDRESS) as session:\n","  print('TPU devices:')\n","  pprint.pprint(session.list_devices())\n","\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","  # Now credentials are set for all future sessions on this TPU."],"execution_count":104,"outputs":[{"output_type":"stream","text":["TPU address is grpc://10.124.108.66:8470\n","TPU devices:\n","[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 16449318307470296195),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3599827788311202100),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 321353889036299835),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9236739624978311139),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6577049995452220479),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10360541729901226601),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3004189358294271403),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9307654225121160783),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 1832903950575003574),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3591665848294123692),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 4858213039421222898)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fTQzH6jJ2WVI","colab_type":"text"},"source":["## Clone the BERT files"]},{"cell_type":"code","metadata":{"id":"1LWNoaPLRTHP","colab_type":"code","colab":{}},"source":["import sys\n","\n","!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n","if not 'bert_repo' in sys.path:\n","  sys.path += ['bert_repo']\n","\n","# import python modules defined by BERT\n","import modeling\n","import optimization\n","import run_classifier\n","import run_classifier_with_tfhub\n","import tokenization\n","\n","# import tfhub \n","import tensorflow_hub as hub"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EiUkXrZV3CZo","colab_type":"text"},"source":["## Authenticat Google Cloud Storage locations for saving files"]},{"cell_type":"code","metadata":{"id":"9CpmEcVUSwJX","colab_type":"code","outputId":"0ac10543-df78-42a4-ce8a-5fb4afca930d","executionInfo":{"status":"ok","timestamp":1572711195038,"user_tz":0,"elapsed":3201,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!pip install gcsfs"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n","Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.5.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth>=1.2->gcsfs) (0.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CYqCfHiHRkO1","colab_type":"code","outputId":"67ebd5da-5830-4a46-b029-3098442579e4","executionInfo":{"status":"ok","timestamp":1572711199350,"user_tz":0,"elapsed":1167,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","FILE_OUTPUT_DIR = 'bert_large_assess_files'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'dissertation_bucket' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  FILE_OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, FILE_OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(FILE_OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(FILE_OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(FILE_OUTPUT_DIR))"],"execution_count":107,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://dissertation_bucket/bert_large_assess_files *****\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C9qhWHpjrVF3","colab_type":"code","outputId":"946bbe89-dfa1-43e1-eeb6-8a43e4515eff","executionInfo":{"status":"ok","timestamp":1572711239401,"user_tz":0,"elapsed":1115,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'bert_large_assess'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'dissertation_bucket' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"],"execution_count":109,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://dissertation_bucket/bert_large_assess *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aniLm6g43Nin","colab_type":"text"},"source":["## Select the BERT model to be used (Base or Large)"]},{"cell_type":"code","metadata":{"id":"o98mBABUsCZd","colab_type":"code","colab":{}},"source":["# Available pretrained model checkpoints:\n","#   uncased_L-12_H-768_A-12: uncased BERT base model\n","#   uncased_L-24_H-1024_A-16: uncased BERT large model\n","#   cased_L-12_H-768_A-12: cased BERT large model\n","BERT_MODEL = 'uncased_L-24_H-1024_A-16' #@param {type:\"string\"}\n","BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q9iLRaaD3Wkm","colab_type":"text"},"source":["## Create a tokenizer"]},{"cell_type":"code","metadata":{"id":"0nDGT3ZYTDO-","colab_type":"code","outputId":"b5941026-98ec-49f8-be15-c97a43466812","executionInfo":{"status":"ok","timestamp":1572711299765,"user_tz":0,"elapsed":54633,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)"],"execution_count":111,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Dh3xwC183l9d","colab_type":"text"},"source":["## Import the Train and Eval datasets from GitHub"]},{"cell_type":"code","metadata":{"id":"_rLXUE-wT462","colab_type":"code","colab":{}},"source":["! wget -cq https://github.com/ScottJK-20190706/tweet_classifier/blob/master/pickle_files//train_data_formatted.pickle?raw=true\n","! wget -cq https://github.com/ScottJK-20190706/tweet_classifier/blob/master/pickle_files/eval_data_formatted.pickle?raw=true\n","\n","import pandas as pd\n","\n","train_data = pd.read_pickle('train_data_formatted.pickle?raw=true')\n","eval_data = pd.read_pickle('eval_data_formatted.pickle?raw=true')\n","all_data = train_data.append(eval_data).reset_index()\n","\n","DATA_COLUMN = 'Tweet'\n","LABEL_COLUMN = 'class'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMrhQUnu3zO_","colab_type":"text"},"source":["## Set up the configurations"]},{"cell_type":"code","metadata":{"id":"1jV6VViDWHdw","colab_type":"code","colab":{}},"source":["TRAIN_BATCH_SIZE = 32\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 4.0\n","MAX_SEQ_LENGTH = 55\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","SAVE_SUMMARY_STEPS = 500\n","\n","#processor = processors[TASK.lower()]()\n","label_list = [0, 1]\n","#label_list = processor.get_labels()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQGyEZTz4rO4","colab_type":"text"},"source":["## 10 stratified splits of the data.  Train model and assess 10 times."]},{"cell_type":"code","metadata":{"id":"fqqcdW5STS9d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pU2KXbjBf5gnoVGVl-r0m3VlZz1JxY5c"},"outputId":"09a01307-a6fe-42ed-96a8-53cc0379b3df","executionInfo":{"status":"ok","timestamp":1572716200433,"user_tz":0,"elapsed":4863316,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}}},"source":["model_run = []\n","name = []\n","tn = []\n","fp = []\n","fn = []\n","tp = []\n","p = []\n","r = []\n","f_1 = []\n","auc_sc = []\n","acc = []\n","\n","n = 0\n","\n","from sklearn.model_selection import StratifiedShuffleSplit\n","sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n","for train_index, test_index in sss.split(all_data['Tweet'], all_data['class']):\n","\n","  # Set the output directory for saving model file\n","  # Optionally, set a GCP bucket location\n","\n","  OUTPUT_DIR = 'bert_large_assess'#@param {type:\"string\"}\n","  #@markdown Whether or not to clear/delete the directory and create a new one\n","  DO_DELETE = True #@param {type:\"boolean\"}\n","  #@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","  USE_BUCKET = True #@param {type:\"boolean\"}\n","  BUCKET = 'dissertation_bucket' #@param {type:\"string\"}\n","\n","  if USE_BUCKET:\n","    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","    from google.colab import auth\n","    auth.authenticate_user()\n","\n","  if DO_DELETE:\n","    try:\n","      tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","    except:\n","      # Doesn't matter if the directory didn't exist\n","      pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","  print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n","\n","\n","  td = all_data.loc[train_index,:]\n","  #td = train_data\n","  ed = all_data.loc[test_index,:]\n","  #ed =\n","  print(td.shape)\n","  print(ed.shape)\n","\n","  # Compute number of train and warmup steps from batch size\n","  # Use the InputExample class from BERT's run_classifier code to create examples from the data\n","  train_examples = td.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                    text_a = x[DATA_COLUMN], \n","                                                                    text_b = None, \n","                                                                    label = x[LABEL_COLUMN]), axis = 1)\n","  num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","  ##### FORMAT THE EVALUATION DATASET #####\n","  eval_examples = ed.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                    text_a = x[DATA_COLUMN], \n","                                                                    text_b = None, \n","                                                                    label = x[LABEL_COLUMN]), axis = 1)\n","\n","  # Setup TPU related config\n","  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","  NUM_TPU_CORES = 8\n","  ITERATIONS_PER_LOOP = 1000\n","\n","  def get_run_config(output_dir):\n","    return tf.contrib.tpu.RunConfig(\n","      cluster=tpu_cluster_resolver,\n","      model_dir=output_dir,\n","      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=ITERATIONS_PER_LOOP,\n","          num_shards=NUM_TPU_CORES,\n","          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","  # Force TF Hub writes to the GS bucket we provide.\n","  os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n","\n","\n","  ##### TENSORFLOW ESTIMATOR OBJECT #####\n","  model_fn = run_classifier_with_tfhub.model_fn_builder(\n","    num_labels=len(label_list),\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    bert_hub_module_handle=BERT_MODEL_HUB\n","  )\n","\n","  estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=get_run_config(OUTPUT_DIR),\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE,\n","  )\n","\n","\n","  # FUNCTION FOR TRAINING THE MODEL #####\n","  def model_train(estimator):\n","        # We'll set sequences to be at most 128 tokens long.\n","    train_features = run_classifier.convert_examples_to_features(\n","        train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    print('***** Started training at {} *****'.format(datetime.datetime.now()))\n","    print('  Num examples = {}'.format(len(train_examples)))\n","    print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n","    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n","    train_input_fn = run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=True)\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","    print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n","\n","\n","  model_train(estimator_from_tfhub)\n","\n","\n","  ##### FUNCTION FOR MAKING PRESICTIONS #####\n","  def model_predict(estimator):\n","    # Make predictions on a subset of eval examples\n","    #prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n","    input_features = run_classifier.convert_examples_to_features(eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","    predictions = estimator.predict(predict_input_fn)\n","    \n","    pred_0 = []\n","    actual = []\n","\n","    for example, prediction in zip(eval_examples, predictions):\n","      print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\n' % (example.text_a, example.text_b, str(example.label), prediction['probabilities']))\n","      pred_0 = np.append(pred_0,prediction['probabilities'])\n","      actual = np.append(actual,example.label)\n","    \n","    return(pred_0, actual)\n","\n","  ##### MAKE THE PREDICTIONS  ##### \n","  pred_0, actual = model_predict(estimator_from_tfhub)\n","\n","\n","  ##### GET THE PREDICTION RESULTS  ##### \n","  i=0\n","  pred = []\n","  while i <= len(pred_0)-1:\n","    if pred_0[i]<=pred_0[i+1]:\n","      pred = np.append(pred,1)\n","    else:\n","      pred = np.append(pred,0)\n","    i = i+2\n","\n","  from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve, accuracy_score #metrics\n","\n","  conf_matrix = confusion_matrix(actual,pred) #build confusion matrix\n","  precision = precision_score(actual,pred) #calculate precision\n","  recall = recall_score(actual,pred) #calculate recall\n","  f1 = f1_score(actual,pred) #calculate f1\n","  fpr, tpr, thresholds = roc_curve(actual,pred)\n","  auc_score = auc(fpr, tpr) #calculate auc\n","  accuracy = accuracy_score(actual,pred) #calculate accuracy\n","\n","  model_run = np.append(model_run,n)\n","  name = np.append(name,'bert_large')\n","  tn = np.append(tn,conf_matrix[0][0])\n","  fp = np.append(fp,conf_matrix[0][1])\n","  fn = np.append(fn,conf_matrix[1][0])\n","  tp = np.append(tp,conf_matrix[1][1])\n","  p = np.append(p,precision)\n","  r = np.append(r,recall)\n","  f_1 = np.append(f_1,f1)\n","  auc_sc = np.append(auc_sc,auc_score)\n","  acc = np.append(acc,accuracy)\n","\n","  n = n+1\n","\n"],"execution_count":113,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"IrPGMZZ1YMHu","colab_type":"code","colab":{}},"source":["metrics = pd.DataFrame({\n","  'model_run': model_run,  \n","  'name' : name,\n","  'tn' : tn,\n","  'fp' : fp,\n","  'fn' : fn,\n","  'tp' : tp,\n","  'p' : p,\n","  'r' : r,\n","  'f_1' : f_1,\n","  'auc_sc' : auc_sc,\n","  'acc' : acc  \n","})\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0oe8UeBkdqB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":359},"outputId":"64ce8c4b-e788-4a73-f379-16cbef80c9bd","executionInfo":{"status":"ok","timestamp":1572720569580,"user_tz":0,"elapsed":470,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}}},"source":["metrics"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_run</th>\n","      <th>name</th>\n","      <th>tn</th>\n","      <th>fp</th>\n","      <th>fn</th>\n","      <th>tp</th>\n","      <th>p</th>\n","      <th>r</th>\n","      <th>f_1</th>\n","      <th>auc_sc</th>\n","      <th>acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>bert_large</td>\n","      <td>845.0</td>\n","      <td>8.0</td>\n","      <td>6.0</td>\n","      <td>162.0</td>\n","      <td>0.952941</td>\n","      <td>0.964286</td>\n","      <td>0.958580</td>\n","      <td>0.977454</td>\n","      <td>0.986288</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>bert_large</td>\n","      <td>838.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>154.0</td>\n","      <td>0.911243</td>\n","      <td>0.916667</td>\n","      <td>0.913947</td>\n","      <td>0.949541</td>\n","      <td>0.971596</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>bert_large</td>\n","      <td>841.0</td>\n","      <td>12.0</td>\n","      <td>24.0</td>\n","      <td>144.0</td>\n","      <td>0.923077</td>\n","      <td>0.857143</td>\n","      <td>0.888889</td>\n","      <td>0.921537</td>\n","      <td>0.964740</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>bert_large</td>\n","      <td>846.0</td>\n","      <td>7.0</td>\n","      <td>14.0</td>\n","      <td>154.0</td>\n","      <td>0.956522</td>\n","      <td>0.916667</td>\n","      <td>0.936170</td>\n","      <td>0.954230</td>\n","      <td>0.979432</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>bert_large</td>\n","      <td>843.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>159.0</td>\n","      <td>0.940828</td>\n","      <td>0.946429</td>\n","      <td>0.943620</td>\n","      <td>0.967353</td>\n","      <td>0.981391</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5.0</td>\n","      <td>bert_large</td>\n","      <td>838.0</td>\n","      <td>15.0</td>\n","      <td>9.0</td>\n","      <td>159.0</td>\n","      <td>0.913793</td>\n","      <td>0.946429</td>\n","      <td>0.929825</td>\n","      <td>0.964422</td>\n","      <td>0.976494</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6.0</td>\n","      <td>bert_large</td>\n","      <td>835.0</td>\n","      <td>18.0</td>\n","      <td>9.0</td>\n","      <td>159.0</td>\n","      <td>0.898305</td>\n","      <td>0.946429</td>\n","      <td>0.921739</td>\n","      <td>0.962663</td>\n","      <td>0.973555</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7.0</td>\n","      <td>bert_large</td>\n","      <td>844.0</td>\n","      <td>9.0</td>\n","      <td>9.0</td>\n","      <td>159.0</td>\n","      <td>0.946429</td>\n","      <td>0.946429</td>\n","      <td>0.946429</td>\n","      <td>0.967939</td>\n","      <td>0.982370</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8.0</td>\n","      <td>bert_large</td>\n","      <td>841.0</td>\n","      <td>12.0</td>\n","      <td>9.0</td>\n","      <td>159.0</td>\n","      <td>0.929825</td>\n","      <td>0.946429</td>\n","      <td>0.938053</td>\n","      <td>0.966180</td>\n","      <td>0.979432</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9.0</td>\n","      <td>bert_large</td>\n","      <td>842.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>156.0</td>\n","      <td>0.934132</td>\n","      <td>0.928571</td>\n","      <td>0.931343</td>\n","      <td>0.957838</td>\n","      <td>0.977473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   model_run        name     tn    fp  ...         r       f_1    auc_sc       acc\n","0        0.0  bert_large  845.0   8.0  ...  0.964286  0.958580  0.977454  0.986288\n","1        1.0  bert_large  838.0  15.0  ...  0.916667  0.913947  0.949541  0.971596\n","2        2.0  bert_large  841.0  12.0  ...  0.857143  0.888889  0.921537  0.964740\n","3        3.0  bert_large  846.0   7.0  ...  0.916667  0.936170  0.954230  0.979432\n","4        4.0  bert_large  843.0  10.0  ...  0.946429  0.943620  0.967353  0.981391\n","5        5.0  bert_large  838.0  15.0  ...  0.946429  0.929825  0.964422  0.976494\n","6        6.0  bert_large  835.0  18.0  ...  0.946429  0.921739  0.962663  0.973555\n","7        7.0  bert_large  844.0   9.0  ...  0.946429  0.946429  0.967939  0.982370\n","8        8.0  bert_large  841.0  12.0  ...  0.946429  0.938053  0.966180  0.979432\n","9        9.0  bert_large  842.0  11.0  ...  0.928571  0.931343  0.957838  0.977473\n","\n","[10 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"46vzsJh458wI","colab_type":"text"},"source":["## Save metrics and predictions"]},{"cell_type":"code","metadata":{"id":"9b63grAYj8Dj","colab_type":"code","outputId":"61fc6993-901b-4cb4-f20a-a2871c8bd50e","executionInfo":{"status":"ok","timestamp":1572720633244,"user_tz":0,"elapsed":4150,"user":{"displayName":"Scott Kilgariff","photoUrl":"","userId":"04893934711112937862"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["metrics.to_csv('gs://dissertation_bucket/bert_large_assess_files/metrics.csv')\n","\n","!gsutil cp /tmp/prediction_summary.pickle gs://dissertation_bucket/bert_large_assess_files/"],"execution_count":116,"outputs":[{"output_type":"stream","text":["WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"],"name":"stderr"},{"output_type":"stream","text":["CommandException: No URLs matched: /tmp/prediction_summary.pickle\n"],"name":"stdout"}]}]}