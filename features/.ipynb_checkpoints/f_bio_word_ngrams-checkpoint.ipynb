{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and evaluation into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(r'C:\\Users\\scott\\Dissertation\\data_sort\\pickle_files\\eval_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle(r'C:\\Users\\scott\\Dissertation\\data_sort\\pickle_files\\train_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Bio'] = all_data['Bio'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6297\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].fillna('')\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Bio'] = [p.sub('', x) for x in all_data['Bio'].tolist()] #remove the punctuation\n",
    "for i in all_data.index:\n",
    "    #print(i)\n",
    "    #all_data.loc[i,'Bio'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        \n",
       "1       official twitter page of the press and journal...\n",
       "2                         ens volem viveslliures i rebels\n",
       "3       up above the streets and houses rainbow climbi...\n",
       "4       conscript all remainers into the eu army engli...\n",
       "5       edinburgh ward 8 councillor working hard to ma...\n",
       "6       its unrealistic to expect me to act like an ad...\n",
       "7       the best part of me ended up a dirty stain on ...\n",
       "8                                                la  wwfc\n",
       "9        cofounder of macrebur  the plastic roads company\n",
       "10      scotlands national newspaper see also scotsman...\n",
       "11                                                       \n",
       "12      proud to be english and a yorkshireman a belie...\n",
       "13                                                    meh\n",
       "14      a veteran of societey a believer in ethics and...\n",
       "15                      freethinking 70 year old hedonist\n",
       "16      protomaker 3d printer   political tweets 3d pr...\n",
       "17                                           brexit party\n",
       "18      classicist and philosopher we need to explore ...\n",
       "19      follow politics uk is a new social media platf...\n",
       "20                                                       \n",
       "21      consectatio excellentiae  in pursuit of excell...\n",
       "22      im a yes supporter and believe scots must end ...\n",
       "23                                                       \n",
       "24      still not sure whats the bigger joke my life o...\n",
       "25      el más grande autor cómico de todos los tiempo...\n",
       "26      tamquam spectator novus  lecturer in internati...\n",
       "27      conservative msp for glasgow politician lawyer...\n",
       "28      love britain sick of eu corruption brexit part...\n",
       "29      scotlands national newspaper see also scotsman...\n",
       "                              ...                        \n",
       "6267    in the end we will remember not the words of o...\n",
       "6268    the good of the scorpion is not the good of th...\n",
       "6269                             no time like the present\n",
       "6270    alternative news  information\\nnews alternativ...\n",
       "6271    im always forced to do stuff im not qualified ...\n",
       "6272    motorbikes partick thistle and scottish indepe...\n",
       "6273    diva of broadwayfulham broadway  proud to be b...\n",
       "6274    english man living in hampsteadlondonenjoy mot...\n",
       "6275    man on a missioni wish in the meantime be good...\n",
       "6276                                                   l \n",
       "6277                          occasionally statistitching\n",
       "6278    scottisheuropean  firmly believe in scotlands ...\n",
       "6279        i love my big diesel car\\nmelt the snowflakes\n",
       "6280                                                     \n",
       "6281    animal lover owned by a cat jack brexiteer ant...\n",
       "6282    rangers lets go history music       i mostly r...\n",
       "6283    voted brexit retired nhs professional backing ...\n",
       "6284    i have a great love for scotland and hope to h...\n",
       "6285    im a person that brings out strong emotions in...\n",
       "6286    lucky to be british and scottish hates nationa...\n",
       "6287                                 university of vienna\n",
       "6288          liverpool fc supporter live and breathe lfc\n",
       "6289    apparently you become a bot when idiots are pr...\n",
       "6290    people may forget what you said they may forge...\n",
       "6291                                                     \n",
       "6292                                                     \n",
       "6293    people with their heads up their backsides onl...\n",
       "6294                      partially titanium neogeo et al\n",
       "6295    our scottish tablet is handmade to a family re...\n",
       "6296    communitarian collectivist  morrissey enthusia...\n",
       "Name: Bio, Length: 6297, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6297\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6297\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(prog, folder, n_gram_size,file):\n",
    "\n",
    "    tweet_ids = [] #initialise list of tweet ids\n",
    "    n_grams = [] #initialise list of n_grams\n",
    "\n",
    "\n",
    "\n",
    "    progress_dumps = prog #memory becomes a problem with this code.  Set a value n whereby the processed lists of tweets will be saved\n",
    "                         #to disk every n tweets and memory will be cleared***\n",
    "    location = folder + '*.pickle'\n",
    "    print(folder)\n",
    "    x = glob.glob(folder)\n",
    "    print(len(x))\n",
    "    if len(x)<2:\n",
    "        for i in all_data.index:\n",
    "\n",
    "            tweet_id = all_data['Tweet ID'][i]                                  #get the tweet id\n",
    "            tokens = all_data.Bio[i]                                          #get the associated tokens\n",
    "            n=n_gram_size                                                   #set the n_gram length***\n",
    "            if len(all_data.Bio[i])==0:\n",
    "                n_grams = np.append(n_grams,'zzzz')                                 #if there are no tokens add \"zzzz\" for this tweet\n",
    "                tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            else:\n",
    "                for c in range(0,len(all_data.Bio[i])-n+1):\n",
    "                    n_gram = ' '.join(all_data.Bio[i][c:c+n])\n",
    "                    n_grams = np.append(n_grams,n_gram)               #add n_gram to list\n",
    "                    tweet_ids = np.append(tweet_ids,tweet_id) \n",
    "\n",
    "            if (i//progress_dumps==i/progress_dumps):                       #save progress and clear memory\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'          #set folder and filename***\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})                       #create dataframe\n",
    "                df.to_pickle(filename)                                      #save\n",
    "                del df                                                      #clear memory\n",
    "                tweet_ids = []                                              #reinitialise list of tweet ids\n",
    "                n_grams = []                                                #reinitialise list of n_grams\n",
    "                print(i)                                                    #print progress\n",
    "\n",
    "            if (i==max(all_data.index)):                                          #as above but for the last chunk of tweets\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})\n",
    "                df.to_pickle(filename)\n",
    "                del df\n",
    "                tweet_ids = []\n",
    "                n_grams = [] \n",
    "                print(i)\n",
    "    else:\n",
    "        print(\"Files already exist in this folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save char ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_size = 4\n",
      "bio_word_4grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6296\n",
      "n_gram_size = 3\n",
      "bio_word_3grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6296\n",
      "n_gram_size = 2\n",
      "bio_word_2grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6296\n",
      "n_gram_size = 1\n",
      "bio_word_1grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6296\n"
     ]
    }
   ],
   "source": [
    "progs = [500,500,500,500]\n",
    "folders = ['bio_word_4grams/','bio_word_3grams/','bio_word_2grams/','bio_word_1grams/']\n",
    "n_gram_sizes = [4,3,2,1]\n",
    "\n",
    "for prog, folder, n_gram_size in zip(progs, folders, n_gram_sizes):\n",
    "    print('n_gram_size = ' + str(n_gram_size))\n",
    "    create_ngrams(prog, folder, n_gram_size, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             n_gram             tweet_id\n",
      "0         im trevor  1150454531282690048\n",
      "1     trevor kingof  1150454531282690048\n",
      "2     kingof winner  1150454531282690048\n",
      "3      winner maker  1150454531282690048\n",
      "4      maker things  1150454531282690048\n",
      "5       things make  1150454531282690048\n",
      "6       make people  1150454531282690048\n",
      "7      people happy  1150454531282690048\n",
      "8         happy dms  1150454531282690048\n",
      "9      dms messages  1150454531282690048\n",
      "10  messages always  1150454531282690048\n",
      "11   always welcome  1150454531282690048\n",
      "12     welcome line  1150454531282690048\n",
      "13          line id  1150454531282690048\n",
      "14  id bloozsingahh  1150454531282690048\n",
      "15        radio tay  1153658341295632384\n",
      "16         tay news  1153658341295632384\n",
      "17    news provides  1153658341295632384\n",
      "18    provides news  1153658341295632384\n",
      "19     news matters  1153658341295632384\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_pickle('bio_word_2grams/progress_2500.pickle')\n",
    "print(x.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
