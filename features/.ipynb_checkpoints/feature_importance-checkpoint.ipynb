{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_matrix(folders):\n",
    "    i=1 #counter\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        location = folder + '*.pickle'\n",
    "        for file in glob.glob(location):                         #loop through each saved file\n",
    "            #print(file)\n",
    "            df = pd.read_pickle(file)                                        #read each file into a dataframe\n",
    "            df['tweet_id'] = df.tweet_id.astype(str)\n",
    "            if (i==1):                                                       #if we're on the first iteration\n",
    "                ngrams = df\n",
    "            else:\n",
    "                ngrams = ngrams.append(df)                             #append if we're not on the first iiteration\n",
    "            i = i+1\n",
    "\n",
    "    ngrams['count'] = 1                                               #add a filed to count occurences\n",
    "    ngrams.columns = ['feature','tweet_ids','value']                  #change the column names\n",
    "\n",
    "    print('aggregating')\n",
    "    features_agg = ngrams.groupby(['tweet_ids', 'feature'])['value'].count().reset_index() #aggregate the full dataframe\n",
    "    x = features_agg\n",
    "    x = x.groupby(['feature'])['value'].sum().reset_index()\n",
    "    print(x.shape)\n",
    "    print(x.columns)\n",
    "    x = x[x['value']>839//20]                                    #term must exist in at least 5% of the smaller class\n",
    "    frequent_features = np.unique(x.feature)\n",
    "    features_agg = features_agg[features_agg.feature.isin(frequent_features)]\n",
    "\n",
    "    count_features = pd.read_pickle('count_features/count_features.pickle') #get the count features\n",
    "    count_features['tweet_ids'] = count_features.tweet_ids.astype(str)\n",
    "\n",
    "    features_agg = features_agg.append(count_features)                          #append count features on to ngrams\n",
    "\n",
    "    print('pivoting')\n",
    "    features_agg_pivot = features_agg.pivot_table(index=['tweet_ids'], columns='feature', values='value').reset_index() #pivot\n",
    "    print('filling nans')\n",
    "    features_agg_pivot.fillna(0, inplace=True)                                #replace nans with zeros\n",
    "    print(features_agg_pivot.shape) #for convenience\n",
    "    print(len(np.unique(features_agg_pivot.tweet_ids)))\n",
    "    return(features_agg_pivot,features_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ngrams matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_word_4grams/\n",
      "tweet_word_3grams/\n",
      "tweet_word_2grams/\n",
      "tweet_word_1grams/\n",
      "tweet_char_4grams/\n",
      "tweet_char_3grams/\n",
      "tweet_char_2grams/\n",
      "tweet_char_1grams/\n",
      "aggregating\n",
      "(124160, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(5104, 3102)\n",
      "5104\n",
      "tweet_word_4grams/\n",
      "tweet_word_3grams/\n",
      "tweet_word_2grams/\n",
      "tweet_word_1grams/\n",
      "tweet_char_4grams/\n",
      "tweet_char_3grams/\n",
      "tweet_char_2grams/\n",
      "tweet_char_1grams/\n",
      "bio_word_4grams/\n",
      "bio_word_3grams/\n",
      "bio_word_2grams/\n",
      "bio_word_1grams/\n",
      "bio_char_4grams/\n",
      "bio_char_3grams/\n",
      "bio_char_2grams/\n",
      "bio_char_1grams/\n",
      "aggregating\n",
      "(229541, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(5104, 5563)\n",
      "5104\n"
     ]
    }
   ],
   "source": [
    "tweet_word_folders = ['tweet_word_4grams/',\n",
    "           'tweet_word_3grams/',\n",
    "           'tweet_word_2grams/',\n",
    "           'tweet_word_1grams/']\n",
    "\n",
    "tweet_char_folders = ['tweet_char_4grams/',\n",
    "           'tweet_char_3grams/',\n",
    "           'tweet_char_2grams/',\n",
    "           'tweet_char_1grams/']\n",
    "\n",
    "bio_word_folders = ['bio_word_4grams/',\n",
    "           'bio_word_3grams/',\n",
    "           'bio_word_2grams/',\n",
    "           'bio_word_1grams/']\n",
    "\n",
    "bio_char_folders = ['bio_char_4grams/',\n",
    "           'bio_char_3grams/',\n",
    "           'bio_char_2grams/',\n",
    "           'bio_char_1grams/']\n",
    "tweet_folders = np.append(tweet_word_folders,tweet_char_folders)\n",
    "tweetbio_folders = np.append(tweet_folders,np.append(bio_word_folders,bio_char_folders))\n",
    "tweet_ngrams_matrix, tweet_ngrams_list = ngram_matrix(tweet_folders) #word/char ngrams in tweets\n",
    "tweetbio_ngrams_matrix, tweetbio_ngrams_list = ngram_matrix(tweetbio_folders) #word/char ngrams in tweets and bios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data to obtain classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(r'C:\\Users\\scott\\Dissertation\\data_sort\\pickle_files\\train_data_formatted.pickle')\n",
    "train_data['Tweet ID'] = train_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "eval_data = pd.read_pickle(r'C:\\Users\\scott\\Dissertation\\data_sort\\pickle_files\\eval_data_formatted.pickle')\n",
    "eval_data['Tweet ID'] = eval_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "tweet_class = all_data.loc[:,['Tweet ID','class']] #get the id and the class column\n",
    "tweet_class.columns = ['Tweet ID','class_column'] #change name of 'class' to 'class_column' ('class' might be a unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for performing Chi^2 test on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2test(df):    \n",
    "    df_class = pd.merge(df,tweet_class, left_on='tweet_ids',right_on='Tweet ID', how='inner') #get the class for instances in df\n",
    "    Y = df_class.loc[:,'class_column'].copy()                                                 #create list of classes\n",
    "    df_class = df_class.drop(['class_column','Tweet ID','tweet_ids'], axis=1)                 #drop class and id features from df\n",
    "    X = df_class.values                                                                       #create matrix of features\n",
    "\n",
    "    from sklearn.feature_selection import chi2\n",
    "    current = datetime.now()\n",
    "    x = chi2(X,Y)                                             #perfrom chi2 test\n",
    "    time_taken = datetime.now() - current\n",
    "    print('time to process chi 2 = ',time_taken)\n",
    "    \n",
    "    important_cols = df_class.columns[x[1]<0.05]              #create list of features where p-value < 0.05\n",
    "    \n",
    "    return(important_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for reducing features based on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df,coeff):\n",
    "    current = datetime.now()\n",
    "    print('Creating correlation matrix')\n",
    "    corr = df.iloc[:,1:].corr()             #create the correlayion matrix\n",
    "    x = datetime.now() - current\n",
    "    print('time to process correlation matrix = ',x)\n",
    "\n",
    "    x = datetime.now() - current\n",
    "    ids = df.iloc[:,:1]                     #create a list for the ids to be added back later\n",
    "    print('number of features = ',len(corr.index))#print the number of features to be analysed\n",
    "    i = 1\n",
    "    for m in corr.index:                          #loop through each row\n",
    "        for n in corr.index:                #loop through each column\n",
    "            if m!=n:                        #ignore if row equals column\n",
    "                try:\n",
    "                    r = corr.loc[m,n]           #get correlation value at intersection\n",
    "                    if r>coeff:                   #if correlation value is greater than 0.9\n",
    "                        corr = corr.drop(n, axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "        if (i//100==i/100):\n",
    "            print('progress = ',str(i))\n",
    "        i = i+1\n",
    "\n",
    "    df = df[corr.columns]\n",
    "    df['tweet_id'] = ids\n",
    "    x = datetime.now() - current\n",
    "    print('time to check all features = ',x)\n",
    "\n",
    "    return(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to combine 'chi2test' and 'correlation' functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2corr(df_matrix):\n",
    "    df_features = chi2test(df_matrix)                                               #get list of features where p<0.05\n",
    "    df_features = np.append('tweet_ids',df_features)                                #append tweet_id to feature list\n",
    "    df_chi = df_matrix[df_features]                                                 #reduce features using chi2test output\n",
    "    df_chi_corr = correlation(df_chi,0.9)                                           #reduce features based on correlation\n",
    "    return(df_chi_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform chi^2 test and corr test and reduce features based on result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process chi 2 =  0:00:00.167690\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:01:27.820689\n",
      "number of features =  1977\n",
      "progress =  100\n",
      "progress =  200\n",
      "progress =  300\n",
      "progress =  400\n",
      "progress =  500\n",
      "progress =  600\n",
      "progress =  700\n",
      "progress =  800\n",
      "progress =  900\n",
      "progress =  1000\n",
      "progress =  1100\n",
      "progress =  1200\n",
      "progress =  1300\n",
      "progress =  1400\n",
      "progress =  1500\n",
      "progress =  1600\n",
      "progress =  1700\n",
      "progress =  1800\n",
      "progress =  1900\n",
      "time to check all features =  0:05:02.656118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "tweet_features = chi2corr(tweet_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process chi 2 =  0:00:00.171833\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:01:23.466773\n",
      "number of features =  2760\n",
      "progress =  100\n",
      "progress =  200\n",
      "progress =  300\n",
      "progress =  400\n",
      "progress =  500\n",
      "progress =  600\n",
      "progress =  700\n",
      "progress =  800\n",
      "progress =  900\n",
      "progress =  1000\n",
      "progress =  1100\n",
      "progress =  1200\n",
      "progress =  1300\n",
      "progress =  1400\n",
      "progress =  1500\n",
      "progress =  1600\n",
      "progress =  1700\n",
      "progress =  1800\n",
      "progress =  1900\n",
      "progress =  2000\n",
      "progress =  2100\n",
      "progress =  2200\n",
      "progress =  2300\n",
      "progress =  2400\n",
      "progress =  2500\n",
      "progress =  2600\n",
      "progress =  2700\n",
      "time to check all features =  0:07:10.995862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "tweetbio_features = chi2corr(tweetbio_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***add code here to add count features to the matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split features matrix into train and eval for tweet-only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features_class = pd.merge(tweet_features,tweet_class,left_on='tweet_id',right_on='Tweet ID')\n",
    "tweet_features_class = tweet_features_class.drop('Tweet ID', axis=1)\n",
    "tweet_features_train = tweet_features_class[tweet_features_class.tweet_id.isin(train_data['Tweet ID'])]\n",
    "tweet_features_train.to_pickle('tweet_features_train.pickle')\n",
    "tweet_features_eval = tweet_features_class[tweet_features_class.tweet_id.isin(eval_data['Tweet ID'])]\n",
    "tweet_features_eval.to_pickle('tweet_features_eval.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split features matrix into train and eval for tweet plus bio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetbio_features_class = pd.merge(tweetbio_features,tweet_class,left_on='tweet_id',right_on='Tweet ID')\n",
    "tweetbio_features_class = tweetbio_features_class.drop('Tweet ID', axis=1)\n",
    "tweetbio_features_train = tweetbio_features_class[tweetbio_features_class.tweet_id.isin(train_data['Tweet ID'])]\n",
    "tweetbio_features_train.to_pickle('tweetbio_features_train.pickle')\n",
    "tweetbio_features_eval = tweetbio_features_class[tweetbio_features_class.tweet_id.isin(eval_data['Tweet ID'])]\n",
    "tweetbio_features_eval.to_pickle('tweetbio_features_eval.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1021, 1888)\n"
     ]
    }
   ],
   "source": [
    "print(tweetbio_features_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
