{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)\n",
      "(41, 12)\n"
     ]
    }
   ],
   "source": [
    "metrics_local = pd.read_pickle('results/metrics_local.pickle')\n",
    "print(metrics_local.shape)\n",
    "metrics_local = metrics_local[~metrics_local.tf.isin(['doc2vec exc_calculated','doc2vec inc_calculated'])]\n",
    "print(metrics_local.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics_local.to_pickle('results/metrics_local.pickle') #pickle the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5104, 16)\n",
      "(5104, 13)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle('pickle_files/eval_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "calculated_features = pd.read_pickle('features/count_features/count_features.pickle') #load calculated features\n",
    "calculated_features = calculated_features.pivot(index='tweet_ids', columns='feature', values='value').reset_index() #pivot\n",
    "calc_feats = ['ave_chars_token', 'caps_count', 'followers_count', 'following_count', #drop columns\n",
    "             'mention_count', 'neg_sent', 'neu_sent', 'pos_sent',\n",
    "             'posted_tweets_count', 'punctuation_count', 'quotes_count', 'url_count',\n",
    "             'tweet_ids']\n",
    "calculated_features = calculated_features[calc_feats]\n",
    "calculated_features['tweet_ids'] = calculated_features['tweet_ids'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "\n",
    "print(all_data.shape)\n",
    "print(calculated_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Tweet_original'] = all_data.Tweet.copy() #keep a copy of the original tweet text\n",
    "all_data['Tweet'] = all_data['Tweet'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].fillna('')\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Tweet'] = [p.sub('', x) for x in all_data['Tweet'].tolist()] #remove the punctuation\n",
    "for i in all_data.index:\n",
    "    #print(i)\n",
    "    #all_data.loc[i,'Tweet'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tagged Gensim documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tweet_docs = [TaggedDocument(doc, [i]) for i, doc in zip(all_data['Tweet ID'],all_data['Tweet'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "vec_size = 300\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=100)\n",
    "model.build_vocab(tweet_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(tweet_docs, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = [] #empty array to hold tweet ids\n",
    "tweet_vectors = [] #empty array to hold document vectors\n",
    "for t_id in all_data['Tweet ID']: #zip through tweets\n",
    "    vector = model.docvecs[t_id] #get vector for each tweet\n",
    "    tweet_ids.append(t_id)\n",
    "    tweet_vectors.append(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4083, 312)\n",
      "(4083, 300)\n",
      "(1021, 312)\n",
      "(1021, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "tweet_df = pd.DataFrame(data=tweet_vectors,                  #convert the document vectors into a pd dataframe\n",
    "          index=np.array(range(0, len(tweet_vectors))),\n",
    "          columns=np.array(range(0, vec_size)))\n",
    "feature_cols = tweet_df.columns.tolist()\n",
    "tweet_df['tweet_id'] = tweet_ids\n",
    "tweet_df['class'] = all_data['class']\n",
    "\n",
    "#merge in the calculated features\n",
    "tweet_df = pd.merge(tweet_df, calculated_features, left_on='tweet_id',right_on='tweet_ids')\n",
    "tweet_df = tweet_df.drop(['tweet_ids'], axis=1)\n",
    "\n",
    "#create train and eval\n",
    "tweet_df_train = tweet_df[tweet_df['tweet_id'].isin(train_data['Tweet ID'].astype(str))]\n",
    "tweet_df_eval = tweet_df[tweet_df['tweet_id'].isin(eval_data['Tweet ID'].astype(str))]\n",
    "\n",
    "tweet_classes = tweet_df_eval.loc[:,['tweet_id','class_column']]\n",
    "\n",
    "calc_feats.remove('tweet_ids') #drop 'tweet_ids from list of calculated features\n",
    "feature_cols = feature_cols + calc_feats #add calc feats to list of included columns\n",
    "\n",
    "x_train = tweet_df_train[feature_cols]\n",
    "print(x_train.shape)\n",
    "x_train_nc = x_train.drop(calc_feats, axis=1)\n",
    "print(x_train_nc.shape)\n",
    "\n",
    "x_train = scale(x_train)\n",
    "x_train_nc = scale(x_train_nc)\n",
    "y_train = tweet_df_train['class']\n",
    "\n",
    "x_eval = tweet_df_eval[feature_cols]\n",
    "print(x_eval.shape)\n",
    "x_eval_nc = x_eval.drop(calc_feats, axis=1)\n",
    "print(x_eval_nc.shape)\n",
    "\n",
    "x_eval = scale(x_eval)\n",
    "x_eval_nc = scale(x_eval_nc)\n",
    "y_eval = tweet_df_eval['class']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifiers and import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #import lr\n",
    "from sklearn.svm import SVC #import svm\n",
    "from sklearn.tree import DecisionTreeClassifier #import dt\n",
    "from sklearn.ensemble import RandomForestClassifier #import rf\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve, accuracy_score #metrics\n",
    "from sklearn.model_selection import GridSearchCV #grid search\n",
    "log_clf = LogisticRegression()\n",
    "svc_clf = SVC()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for fine tuning and evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_grid(classifier,model, x_train, y_train, x_eval, y_eval, tweet_classes):\n",
    "    \n",
    "    if model == 'lr': #if using logisitic regression\n",
    "        param_grid = [{'random_state':[42],\n",
    "               'C':[0.05,0.1,0.5,1],\n",
    "               'penalty':['l1','l2']}]\n",
    "        \n",
    "    if model == 'dt': #if using decision tree\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}]\n",
    "        \n",
    "    if model == 'rf': #if using random forest\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}] \n",
    "    \n",
    "    if model == 'svm': #if using svm\n",
    "        param_grid = [{'random_state':[42],\n",
    "                   'C':[0.05,0.1,1,10], \n",
    "                   'kernel':['linear','rbf']}]\n",
    "    \n",
    "   \n",
    "    param_grid = param_grid\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, scoring='recall') #grid search using 10-folds cross validation\n",
    "    grid_search.fit(x_train, y_train) #fir grid search\n",
    "    print(\"\")\n",
    "    print('Best parameters')\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(best_parameters) #print best parameters from grid search\n",
    "    print('Best grid search score = ',grid_search.best_score_) #print best grid search score\n",
    "    print(\"\")\n",
    "    print('Evaluation data scores')\n",
    "    tuned_clf = grid_search.best_estimator_ #build model using best parameters\n",
    "    tuned_clf_pred = tuned_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "    conf_matrix = confusion_matrix(y_eval,tuned_clf_pred) #build confusion matrix\n",
    "    precision = precision_score(y_eval,tuned_clf_pred) #calculate precision\n",
    "    recall = recall_score(y_eval,tuned_clf_pred) #calculate recall\n",
    "    f1 = f1_score(y_eval,tuned_clf_pred) #calculate f1\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval,tuned_clf_pred)\n",
    "    auc_score = auc(fpr, tpr) #calculate auc\n",
    "    accuracy = accuracy_score(y_eval,tuned_clf_pred) #calculate accuracy\n",
    "    class_eval = tweet_classes.copy()\n",
    "    class_eval['pred'] = tuned_clf_pred\n",
    "    class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "    print(conf_matrix)\n",
    "    print('precision = ' + str(precision))\n",
    "    print('recall = ' + str(recall))\n",
    "    print('f1 = ' + str(f1))\n",
    "    print('auc = ' + str(auc_score))\n",
    "    print('accuracy = ' + str(accuracy))\n",
    "    \n",
    "    return(best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval) #return metrics and pred vs actuals for each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the \"search_grid()\" function for lr, dt, rf, and svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "{'C': 1, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.7301771887187023\n",
      "\n",
      "Evaluation data scores\n",
      "[[800  31]\n",
      " [ 43 147]]\n",
      "precision = 0.8258426966292135\n",
      "recall = 0.7736842105263158\n",
      "f1 = 0.7989130434782608\n",
      "auc = 0.8681898790297041\n",
      "accuracy = 0.9275220372184133\n",
      "\n",
      "Best parameters\n",
      "{'C': 1, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.6008386673637409\n",
      "\n",
      "Evaluation data scores\n",
      "[[798  33]\n",
      " [ 78 112]]\n",
      "precision = 0.7724137931034483\n",
      "recall = 0.5894736842105263\n",
      "f1 = 0.6686567164179105\n",
      "auc = 0.7748812464373931\n",
      "accuracy = 0.89128305582762\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.6300414829782023\n",
      "\n",
      "Evaluation data scores\n",
      "[[768  63]\n",
      " [ 72 118]]\n",
      "precision = 0.6519337016574586\n",
      "recall = 0.6210526315789474\n",
      "f1 = 0.6361185983827493\n",
      "auc = 0.7726201786053583\n",
      "accuracy = 0.8677766895200784\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.5546204713728594\n",
      "\n",
      "Evaluation data scores\n",
      "[[746  85]\n",
      " [ 92  98]]\n",
      "precision = 0.5355191256830601\n",
      "recall = 0.5157894736842106\n",
      "f1 = 0.5254691689008043\n",
      "auc = 0.7067515358794099\n",
      "accuracy = 0.8266405484818805\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.44368064582980093\n",
      "\n",
      "Evaluation data scores\n",
      "[[816  15]\n",
      " [109  81]]\n",
      "precision = 0.84375\n",
      "recall = 0.4263157894736842\n",
      "f1 = 0.5664335664335665\n",
      "auc = 0.7041326239787193\n",
      "accuracy = 0.8785504407443683\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.40205937140865505\n",
      "\n",
      "Evaluation data scores\n",
      "[[821  10]\n",
      " [111  79]]\n",
      "precision = 0.8876404494382022\n",
      "recall = 0.41578947368421054\n",
      "f1 = 0.5663082437275986\n",
      "auc = 0.7018778896700234\n",
      "accuracy = 0.881488736532811\n",
      "\n",
      "Best parameters\n",
      "{'C': 10, 'kernel': 'linear', 'random_state': 42}\n",
      "Best grid search score =  0.7486628421786394\n",
      "\n",
      "Evaluation data scores\n",
      "[[776  55]\n",
      " [ 41 149]]\n",
      "precision = 0.7303921568627451\n",
      "recall = 0.7842105263157895\n",
      "f1 = 0.7563451776649744\n",
      "auc = 0.8590126037114447\n",
      "accuracy = 0.9059745347698335\n",
      "\n",
      "Best parameters\n",
      "{'C': 10, 'kernel': 'linear', 'random_state': 42}\n",
      "Best grid search score =  0.6269719427645585\n",
      "\n",
      "Evaluation data scores\n",
      "[[784  47]\n",
      " [ 69 121]]\n",
      "precision = 0.7202380952380952\n",
      "recall = 0.6368421052631579\n",
      "f1 = 0.6759776536312848\n",
      "auc = 0.7901418709227944\n",
      "accuracy = 0.8863858961802155\n",
      "time taken =  0:28:53.910048\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "current = datetime.now() #for checking duration\n",
    "\n",
    "tf = []     #initialise empty vectors to hold results\n",
    "name = []\n",
    "bp = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "tp = []\n",
    "p = []\n",
    "r = []\n",
    "f_1 = []\n",
    "auc_sc = []\n",
    "acc = []\n",
    "\n",
    "tweet_classes = tweet_df_eval.loc[:,['tweet_id','class_column']]\n",
    "\n",
    "classifiers = [log_clf, dt_clf, rf_clf, svc_clf] #the classifiers that are to be tested\n",
    "models = ['lr','dt','rf','svm'] #labels for identifying the results\n",
    "\n",
    "representation = ['inc_calculated','exc_calculated']\n",
    "train_files_x = [x_train,x_train_nc]\n",
    "train_files_y = [y_train,y_train]\n",
    "\n",
    "eval_files_x = [x_eval,x_eval_nc]\n",
    "eval_files_y = [y_eval,y_eval]\n",
    "\n",
    "i=1 #a counter to be used for checking loop number\n",
    "for classifier, model in zip(classifiers, models): #zip through the classifiers and model names\n",
    "    for rep,tx,ty,ex,ey in zip(representation,train_files_x, train_files_y, eval_files_x, eval_files_y): #zip through the training and evaluation combos\n",
    "        #execute the search_grid() function\n",
    "        best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval = search_grid(classifier,model, tx,ty,ex,ey, tweet_classes)\n",
    "        \n",
    "        #append the latest results to the vectors\n",
    "        tf = np.append(tf,'doc2vec ' + rep)\n",
    "        name = np.append(name,model)\n",
    "        b = ';'.join('{} {}'.format(key, val) for key, val in best_parameters.items())\n",
    "        bp = np.append(bp,b)\n",
    "        tn = np.append(tn,conf_matrix[0][0])\n",
    "        fp = np.append(fp,conf_matrix[0][1])\n",
    "        fn = np.append(fn,conf_matrix[1][0])\n",
    "        tp = np.append(tp,conf_matrix[1][1])\n",
    "        p = np.append(p,precision)\n",
    "        r = np.append(r,recall)\n",
    "        f_1 = np.append(f_1,f1)\n",
    "        auc_sc = np.append(auc_sc,auc_score)\n",
    "        acc = np.append(acc,accuracy)\n",
    "        \n",
    "        #col = train_file+'_'+model #build a column name\n",
    "        #class_eval.columns = ['tweet_id',col] #rename the columns\n",
    "        class_eval['model'] = model\n",
    "        class_eval['file'] = 'doc2vec ' + rep\n",
    "        if i==1: #if we are on the first iteration of the loop\n",
    "            df = class_eval.copy()\n",
    "        else: #if we are not on the first iteration f the loop\n",
    "            #df = pd.merge(df, class_eval, on='tweet_id')\n",
    "            df = df.append(class_eval)\n",
    "        \n",
    "        i = i+1 #increment i\n",
    "\n",
    "print('time taken = ',datetime.now() - current) #print the time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the results so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>name</th>\n",
       "      <th>bp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f_1</th>\n",
       "      <th>auc_sc</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>doc2vec inc_calculated</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 1;penalty l2;random_state 42</td>\n",
       "      <td>800.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.798913</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0.927522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>doc2vec inc_calculated</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 10;kernel linear;random_state 42</td>\n",
       "      <td>776.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.730392</td>\n",
       "      <td>0.784211</td>\n",
       "      <td>0.756345</td>\n",
       "      <td>0.859013</td>\n",
       "      <td>0.905975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>doc2vec exc_calculated</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 10;kernel linear;random_state 42</td>\n",
       "      <td>784.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.720238</td>\n",
       "      <td>0.636842</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.790142</td>\n",
       "      <td>0.886386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>doc2vec exc_calculated</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 1;penalty l2;random_state 42</td>\n",
       "      <td>798.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.668657</td>\n",
       "      <td>0.774881</td>\n",
       "      <td>0.891283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>doc2vec inc_calculated</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>768.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.636119</td>\n",
       "      <td>0.772620</td>\n",
       "      <td>0.867777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>doc2vec inc_calculated</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>816.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.566434</td>\n",
       "      <td>0.704133</td>\n",
       "      <td>0.878550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>doc2vec exc_calculated</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>821.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.415789</td>\n",
       "      <td>0.566308</td>\n",
       "      <td>0.701878</td>\n",
       "      <td>0.881489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>doc2vec exc_calculated</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>746.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.535519</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.525469</td>\n",
       "      <td>0.706752</td>\n",
       "      <td>0.826641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tf name                                  bp     tn  \\\n",
       "0  doc2vec inc_calculated   lr      C 1;penalty l2;random_state 42  800.0   \n",
       "6  doc2vec inc_calculated  svm  C 10;kernel linear;random_state 42  776.0   \n",
       "7  doc2vec exc_calculated  svm  C 10;kernel linear;random_state 42  784.0   \n",
       "1  doc2vec exc_calculated   lr      C 1;penalty l2;random_state 42  798.0   \n",
       "2  doc2vec inc_calculated   dt   criterion entropy;random_state 42  768.0   \n",
       "4  doc2vec inc_calculated   rf   criterion entropy;random_state 42  816.0   \n",
       "5  doc2vec exc_calculated   rf   criterion entropy;random_state 42  821.0   \n",
       "3  doc2vec exc_calculated   dt   criterion entropy;random_state 42  746.0   \n",
       "\n",
       "     fp     fn     tp         p         r       f_1    auc_sc       acc  \n",
       "0  31.0   43.0  147.0  0.825843  0.773684  0.798913  0.868190  0.927522  \n",
       "6  55.0   41.0  149.0  0.730392  0.784211  0.756345  0.859013  0.905975  \n",
       "7  47.0   69.0  121.0  0.720238  0.636842  0.675978  0.790142  0.886386  \n",
       "1  33.0   78.0  112.0  0.772414  0.589474  0.668657  0.774881  0.891283  \n",
       "2  63.0   72.0  118.0  0.651934  0.621053  0.636119  0.772620  0.867777  \n",
       "4  15.0  109.0   81.0  0.843750  0.426316  0.566434  0.704133  0.878550  \n",
       "5  10.0  111.0   79.0  0.887640  0.415789  0.566308  0.701878  0.881489  \n",
       "3  85.0   92.0   98.0  0.535519  0.515789  0.525469  0.706752  0.826641  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = pd.DataFrame({'tf':tf, #create a dataframe to hold the metrics\n",
    "                                'name':name,\n",
    "                               'bp':bp,\n",
    "                               'tn':tn,\n",
    "                               'fp':fp,\n",
    "                               'fn':fn,\n",
    "                               'tp':tp,\n",
    "                               'p':p,\n",
    "                               'r':r,\n",
    "                               'f_1':f_1,\n",
    "                               'auc_sc':auc_sc,\n",
    "                               'acc':acc})\n",
    "\n",
    "classifications.sort_values(by='f_1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[812  19]\n",
      " [ 59 131]]\n",
      "precision = 0.8733333333333333\n",
      "recall = 0.6894736842105263\n",
      "f1 = 0.7705882352941177\n",
      "auc = 0.8333048324783078\n",
      "accuracy = 0.9236043095004897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(penalty='l2',C=1, random_state=42) #logistic regression with best hyperparameters\n",
    "svc_clf = SVC(C=10, kernel='linear', probability = True, random_state=42) #svm with best hyperparameters\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', random_state=42) #random forest with best hyperparameters\n",
    "rf_clf = RandomForestClassifier(criterion='entropy', random_state=42) #random forest with best hyperparameters\n",
    "\n",
    "#create the ensemble\n",
    "e_clf = VotingClassifier(estimators=[('lr', log_clf), ('svm', svc_clf), ('rf', rf_clf), ('dt', dt_clf)],\n",
    "                         voting='soft', weights=[1, 1, 1, 1])\n",
    "\n",
    "e_clf = e_clf.fit(x_train, y_train) #fit the ensemble\n",
    "\n",
    "e_clf_pred = e_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "conf_matrix = confusion_matrix(y_eval,e_clf_pred) #build confusion matrix\n",
    "precision = precision_score(y_eval,e_clf_pred) #calculate precision\n",
    "recall = recall_score(y_eval,e_clf_pred) #calculate recall\n",
    "f1 = f1_score(y_eval,e_clf_pred) #calculate f1\n",
    "fpr, tpr, thresholds = roc_curve(y_eval,e_clf_pred)\n",
    "auc_score = auc(fpr, tpr) #calculate auc\n",
    "accuracy = accuracy_score(y_eval,e_clf_pred) #calculate accuracy\n",
    "class_eval = tweet_classes.copy()\n",
    "class_eval['pred'] = e_clf_pred\n",
    "class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "print(conf_matrix)\n",
    "print('precision = ' + str(precision))\n",
    "print('recall = ' + str(recall))\n",
    "print('f1 = ' + str(f1))\n",
    "print('auc = ' + str(auc_score))\n",
    "print('accuracy = ' + str(accuracy))\n",
    "\n",
    "#append the latest results to the vectors\n",
    "tf = np.append(tf,'doc2vec inc_calculated')\n",
    "name = np.append(name,'ensemble (lr,svc,dt,rf)')\n",
    "bp = np.append(bp,'ensemble')\n",
    "tn = np.append(tn,conf_matrix[0][0])\n",
    "fp = np.append(fp,conf_matrix[0][1])\n",
    "fn = np.append(fn,conf_matrix[1][0])\n",
    "tp = np.append(tp,conf_matrix[1][1])\n",
    "p = np.append(p,precision)\n",
    "r = np.append(r,recall)\n",
    "f_1 = np.append(f_1,f1)\n",
    "auc_sc = np.append(auc_sc,auc_score)\n",
    "acc = np.append(acc,accuracy)\n",
    "\n",
    "class_eval['model'] = 'ensemble (lr,svc,dt,rf)'\n",
    "class_eval['file'] = 'doc2vec inc_calculated'\n",
    "\n",
    "df = df.append(class_eval) #merge the latest predictions for each tweet using this classifier\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append results to existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = pd.DataFrame({'tf':tf, #create a dataframe to hold the metrics\n",
    "                                'name':name,\n",
    "                               'bp':bp,\n",
    "                               'tn':tn,\n",
    "                               'fp':fp,\n",
    "                               'fn':fn,\n",
    "                               'tp':tp,\n",
    "                               'p':p,\n",
    "                               'r':r,\n",
    "                               'f_1':f_1,\n",
    "                               'auc_sc':auc_sc,\n",
    "                               'acc':acc})\n",
    "\n",
    "classifications.sort_values(by='f_1', ascending=False)\n",
    "\n",
    "metrics_local = pd.read_pickle('results/metrics_local.pickle')\n",
    "metrics_local = metrics_local.append(classifications)\n",
    "metrics_local.to_pickle('results/metrics_local.pickle') #pickle the metrics\n",
    "\n",
    "preds_per_tweet_local = pd.read_pickle('results/preds_per_tweet_local.pickle')\n",
    "preds_per_tweet_local = preds_per_tweet_local.append(df)\n",
    "preds_per_tweet_local.to_pickle('results/preds_per_tweet_local.pickle') #pickle the results of actual vs predicted for each tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
