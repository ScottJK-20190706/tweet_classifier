{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and evaluation into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('pickle_files/eval_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Tweet'] = [p.sub('', x) for x in all_data['Tweet'].tolist()] #remove the punctuation\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for i in all_data.index:\n",
    "    for w in range(0,len(all_data.Tweet[i])):\n",
    "        all_data.Tweet[i][w] = ps.stem(all_data.Tweet[i][w])\n",
    "\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in all_data.index:\n",
    "    all_data.Tweet[i] = ' '.join(all_data.Tweet[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding char ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(prog, folder, n_gram_size,file):\n",
    "\n",
    "    tweet_ids = [] #initialise list of tweet ids\n",
    "    n_grams = [] #initialise list of n_grams\n",
    "\n",
    "    n = n_gram_size\n",
    "\n",
    "    progress_dumps = prog #memory becomes a problem with this code.  Set a value n whereby the processed lists of tweets will be saved\n",
    "                         #to disk every n tweets and memory will be cleared***\n",
    "    location = folder + '*.pickle'\n",
    "    print(folder)\n",
    "    x = glob.glob(folder)\n",
    "    print(len(x))\n",
    "    if len(x)<2:\n",
    "        for i in all_data.index:\n",
    "\n",
    "            tweet_id = all_data['Tweet ID'][i]\n",
    "            all_data.Tweet[i] = all_data.Tweet[i].replace(\" \", \"_\")            #replace whitespace with underscore\n",
    "            if len(all_data.Tweet[i])==0:\n",
    "                n_grams = np.append(n_grams,'zzzz')                                 #if there are no tokens add \"zzzz\" for this tweet\n",
    "                tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            else:\n",
    "                for c in range(0,len(all_data.Tweet[i])-n+1):                           #iterate along the length of each token\n",
    "                    n_grams = np.append(n_grams,all_data.Tweet[i][c:c+n])               #add n_gram to list\n",
    "                    tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            if (i//progress_dumps==i/progress_dumps):                       #save progress and clear memory\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'          #set folder and filename***\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})                       #create dataframe\n",
    "                df.to_pickle(filename)                                      #save\n",
    "                del df                                                      #clear memory\n",
    "                tweet_ids = []                                              #reinitialise list of tweet ids\n",
    "                n_grams = []                                                #reinitialise list of n_grams\n",
    "                print(i)                                                    #print progress\n",
    "\n",
    "            if (i==max(all_data.index)):                                          #as above but for the last chunk of tweets\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})\n",
    "                df.to_pickle(filename)\n",
    "                del df\n",
    "                tweet_ids = []\n",
    "                n_grams = [] \n",
    "                print(i)\n",
    "    else:\n",
    "        print(\"Files already exist in this folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save char ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_size = 4\n",
      "features/tweet_char_4grams/\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 3\n",
      "features/tweet_char_3grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 2\n",
      "features/tweet_char_2grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 1\n",
      "features/tweet_char_1grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n"
     ]
    }
   ],
   "source": [
    "progs = [500,500,500,500]\n",
    "folders = ['features/tweet_char_4grams/','features/tweet_char_3grams/','features/tweet_char_2grams/','features/tweet_char_1grams/']\n",
    "n_gram_sizes = [4,3,2,1]\n",
    "\n",
    "for prog, folder, n_gram_size in zip(progs, folders, n_gram_sizes):\n",
    "    print('n_gram_size = ' + str(n_gram_size))\n",
    "    create_ngrams(prog, folder, n_gram_size, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
