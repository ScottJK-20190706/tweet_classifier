{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code obtained from http://socialmedia-class.org/twittertutorial.html\n",
    "#api docs at https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter api tokens and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '<api key goes here>'\n",
    "api_secret_key = '<api secret key goes here>'\n",
    "access_token = '<access token goes here>'\n",
    "access_token_secret = '<access token secret goes here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "# wait_on_rate_limit= True;  will make the api to automatically wait for rate limits to replenish\n",
    "# wait_on_rate_limit_notify= Ture;  will make the api  to print a notification when Tweepyis waiting for rate limits to replenish\n",
    "#---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hate_tweets(listOfTweets, ids):\n",
    "    tweets = api.statuses_lookup(ids, tweet_mode='extended')\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        try:\n",
    "            status = tweets[i]\n",
    "            json_str = json.dumps(status._json)\n",
    "            json_str = json.loads(json_str)\n",
    "            \n",
    "            origin_id = json_str['id']\n",
    "            id = json_str['retweeted_status']['id']\n",
    "            screen_name = json_str['retweeted_status']['user']['screen_name']\n",
    "            user_name = json_str['retweeted_status']['user']['name']\n",
    "            followers = json_str['retweeted_status']['user']['followers_count']\n",
    "            following = json_str['retweeted_status']['user']['friends_count']\n",
    "            tweet_created_at = json_str['retweeted_status']['created_at']\n",
    "            tweet_text = json_str['retweeted_status']['full_text']\n",
    "            bio = json_str['retweeted_status']['user']['description']\n",
    "            location = json_str['retweeted_status']['user']['location']\n",
    "            coord = json_str['retweeted_status']['coordinates']\n",
    "            no_tweets = json_str['retweeted_status']['user']['statuses_count']\n",
    "\n",
    "            if len(json_str['retweeted_status']['entities']['hashtags'])==0:\n",
    "                hashtags = ''\n",
    "            else:\n",
    "                hashtags = []\n",
    "                for i in range(0,len(json_str['retweeted_status']['entities']['hashtags'])):\n",
    "                    hashtags = pd.Series(np.append(hashtags,json_str['retweeted_status']['entities']['hashtags'][i]['text']))\n",
    "                    hashtags = hashtags.str.cat(sep=' ')\n",
    "\n",
    "            if len(json_str['retweeted_status']['entities']['urls'])==0:\n",
    "                urls=''\n",
    "            else:\n",
    "                urls = []\n",
    "                for i in range(0,len(json_str['retweeted_status']['entities']['urls'])):\n",
    "                    urls = pd.Series(np.append(urls,json_str['retweeted_status']['entities']['urls'][i]['url']))\n",
    "                    urls = urls.str.cat(sep=' ')\n",
    "\n",
    "            if len(json_str['retweeted_status']['entities']['user_mentions'])==0:\n",
    "                mentions = ''\n",
    "            else:\n",
    "                mentions = []\n",
    "                for i in range(0,len(json_str['retweeted_status']['entities']['user_mentions'])):\n",
    "                    mentions = pd.Series(np.append(mentions,json_str['retweeted_status']['entities']['user_mentions'][i]['screen_name']))\n",
    "                    mentions = mentions.str.cat(sep=' ')\n",
    "\n",
    "            # Add tweets in this format\n",
    "            dict_ = {'Tweet ID': origin_id,\n",
    "                     'Retweeted ID':id,\n",
    "                    'Name': screen_name,\n",
    "                    'Username': user_name,\n",
    "                    'Number of Followers': followers,\n",
    "                    'Number Following': following,\n",
    "                    'Time': tweet_created_at,\n",
    "                    'Tweet': tweet_text,\n",
    "                     'Hashtags': hashtags,\n",
    "                     'Web': urls,\n",
    "                     'Bio': bio,\n",
    "                     'Location': location,\n",
    "                     'Location Coordinates': coord,\n",
    "                     'Number of Tweets': no_tweets,\n",
    "                     'Mentions': mentions\n",
    "                    }\n",
    "            listOfTweets.append(dict_)\n",
    "        except:\n",
    "            pass\n",
    "    return listOfTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfTweets = 100\n",
    "\n",
    "data = pd.read_csv('cps_cat4.txt', sep=\" \", header=None) #read list of hateful tweet ids\n",
    "loops = data.shape[0]//numOfTweets+1 #define number of required loops\n",
    "for n in range(0,loops):\n",
    "    listOfTweets=[]\n",
    "    ids = data.iloc[n*100:(n+1)*100,0].tolist() #loop through tweetd ids.  100 at a time\n",
    "    x = hate_tweets(listOfTweets, ids) #call the hate_tweets function\n",
    "    df = pd.DataFrame(x) #convert to dataframe\n",
    "    if n==0:\n",
    "        existing_hate = df\n",
    "    else:\n",
    "        existing_hate = existing_hate.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_hate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not all tweets are still available.  Supplement removed tweets with nvivo export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hate = pd.read_excel('csv_excel/full_nvivo_export.xlsx',sheet_name='Table1-1') #load nvivo export xlsx file\n",
    "all_hate = all_hate[~all_hate['Tweet ID'].isin(existing_hate['Tweet ID'])] #remove tweets from nvivo file that were found on twitter\n",
    "all_hate['Retweeted ID'] = '' #add column to nvivo file\n",
    "all_hate = all_hate.drop(['Row ID','Tweet Type','Retweeted By','Number of Retweets'], axis=1) #emove columns from nvivo file\n",
    "existing_hate = existing_hate.append(all_hate) #append nvivo file to file of tweets found on twitter\n",
    "existing_hate = existing_hate[existing_hate['Tweet ID'].isin(data.iloc[:,0].tolist())] #only include tweets identified as hateful\n",
    "existing_hate.to_pickle('pickle_files/existing_hate.pickle') #pickle dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_hate.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
