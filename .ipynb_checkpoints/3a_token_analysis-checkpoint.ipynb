{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839\n",
      "5458\n"
     ]
    }
   ],
   "source": [
    "offensive = pd.read_pickle('pickle_files/offensive_formatted.pickle')\n",
    "benign = pd.read_pickle('pickle_files/benign_formatted.pickle')\n",
    "\n",
    "offensive = offensive.reset_index(drop=True)\n",
    "offensive['Tweet ID'] = offensive['Tweet ID'].astype(str)\n",
    "\n",
    "benign = benign.reset_index(drop=True)\n",
    "benign['Tweet ID'] = benign['Tweet ID'].astype(str)\n",
    "\n",
    "print(len(np.unique(offensive['Tweet ID']))) #for convenience\n",
    "print(len(np.unique(benign['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839\n",
      "5458\n"
     ]
    }
   ],
   "source": [
    "offensive['Tweet_original'] = offensive.Tweet.copy() #keep a copy of the original tweet text\n",
    "offensive['Tweet'] = offensive['Tweet'].str.lower() #lowercase the text \n",
    "\n",
    "benign['Tweet_original'] = benign.Tweet.copy() #keep a copy of the original tweet text\n",
    "benign['Tweet'] = benign['Tweet'].str.lower() #lowercase the text \n",
    "\n",
    "print(len(np.unique(offensive['Tweet ID']))) #for convenience\n",
    "print(len(np.unique(benign['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839\n",
      "5458\n"
     ]
    }
   ],
   "source": [
    "offensive['Tweet'] = offensive['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(offensive['Tweet ID']))) #for convenience\n",
    "\n",
    "benign['Tweet'] = benign['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(benign['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df):\n",
    "    df['Tweet'] = df['Tweet'].fillna('')\n",
    "    p = re.compile(r'[^\\w\\s]+')\n",
    "    df['Tweet'] = [p.sub('', x) for x in df['Tweet'].tolist()] #remove the punctuation\n",
    "    for i in df.index:\n",
    "        #print(i)\n",
    "        #all_data.loc[i,'Bio'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Bio'])\n",
    "        df.loc[i,'Tweet'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", df.loc[i,'Tweet'])\n",
    "        df.loc[i,'Tweet'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", df.loc[i,'Tweet'])\n",
    "        df.loc[i,'Tweet'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", df.loc[i,'Tweet'])\n",
    "    return(df)\n",
    "\n",
    "offensive = remove_punctuation(offensive)\n",
    "benign = remove_punctuation(benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get frequency of unigrams and bigrams per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(df):\n",
    "    unigrams = nltk.word_tokenize(' '.join(df['Tweet']))\n",
    "    bigrams = ngrams(unigrams,2)\n",
    "    unigrams_c = Counter(unigrams)\n",
    "    bigrams_c = Counter(bigrams)\n",
    "\n",
    "    term = []\n",
    "    count = []\n",
    "    for i in bigrams_c:\n",
    "        term = np.append(term,' '.join(i))\n",
    "        count = np.append(count,bigrams_c[i]/df.shape[0])\n",
    "    for i in unigrams_c:\n",
    "        term = np.append(term,i)\n",
    "        count = np.append(count,unigrams_c[i]/df.shape[0])\n",
    "    terms = pd.DataFrame({'term':term,'count':count})\n",
    "    return(terms)\n",
    "offensive_terms = term_count(offensive)\n",
    "benign_terms = term_count(benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table of top 20 unigrams/bigrams per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              benign_term  benign_value   offensive_term  offensive_value\n",
      "0                sturgeon      0.852510         sturgeon         0.927294\n",
      "1                  nicola      0.674240           nicola         0.574493\n",
      "2         nicola sturgeon      0.633565  nicola sturgeon         0.538737\n",
      "3                scotland      0.214181          fucking         0.103695\n",
      "4                   boris      0.185599             face         0.098927\n",
      "5                 johnson      0.154819             fuck         0.098927\n",
      "6                scottish      0.140344             cunt         0.094160\n",
      "7            independence      0.129901            would         0.089392\n",
      "8                  brexit      0.122023            punch         0.077473\n",
      "9           boris johnson      0.116709             like         0.076281\n",
      "10                    snp      0.109930            needs         0.076281\n",
      "11                    not      0.101502             want         0.072706\n",
      "12                     uk      0.101319            bitch         0.067938\n",
      "13               minister      0.089960         scotland         0.061979\n",
      "14                   vote      0.087944              get         0.051251\n",
      "15                  first      0.081348              wee         0.048868\n",
      "16                    new      0.069439   sturgeon needs         0.047676\n",
      "17  scottish independence      0.063027             love         0.047676\n",
      "18                     eu      0.062111              not         0.046484\n",
      "19         first minister      0.057897             ugly         0.045292\n"
     ]
    }
   ],
   "source": [
    "offensive_terms = offensive_terms.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "benign_terms = benign_terms.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "terms = pd.DataFrame({'offensive_term':offensive_terms.term[0:20],\n",
    "                      'offensive_value':offensive_terms['count'][0:20],\n",
    "                      'benign_term':benign_terms.term[0:20],\n",
    "                      'benign_value':benign_terms['count'][0:20]})\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms.to_csv('csv_excel/terms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ids for boris johnson related benign tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bj_ids = []\n",
    "for t in ['boris','johnson','boris johnson']: #loop through each hatebase term\n",
    "    try:\n",
    "        bj_benign = benign[benign.Tweet.str.contains(t)]\n",
    "        bj_ids = np.append(bj_ids,bj_benign['Tweet ID']) #add the index if the tweet contains a hatebase term\n",
    "    except:\n",
    "        pass\n",
    "bj_ids = np.unique(bj_ids) #remove the duplicates\n",
    "len(bj_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove boris johnson related tweets from benign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5458, 14)\n",
      "(4265, 14)\n"
     ]
    }
   ],
   "source": [
    "benign = pd.read_pickle('pickle_files/benign_formatted.pickle')\n",
    "benign['Tweet ID'] = benign['Tweet ID'].astype(str)\n",
    "print(benign.shape)\n",
    "benign = benign[~benign['Tweet ID'].isin(bj_ids)]\n",
    "print(benign.shape)\n",
    "benign.to_pickle('pickle_files/benign_minusbj_formatted.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
