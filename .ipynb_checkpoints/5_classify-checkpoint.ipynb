{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data and class inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xysplit(file_location):\n",
    "    df = pd.read_pickle(file_location) #read file\n",
    "    df['tweet_id'] = df.tweet_id.astype(str) #change tweet_id to string\n",
    "    df = shuffle(df, random_state=42)\n",
    "    df_class = df.loc[:,['tweet_id','class_column']] #create a df of classes per tweet_id\n",
    "    x_df = df.drop(['tweet_id','class_column'], axis=1).values #drop tweet_id and class\n",
    "    x_df = scale(x_df) #scale the data\n",
    "    y_df = df_class.class_column.values #obtain a vector of classes\n",
    "    return(x_df,y_df, df_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifiers and import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #import lr\n",
    "from sklearn.svm import SVC #import svm\n",
    "from sklearn.tree import DecisionTreeClassifier #import dt\n",
    "from sklearn.ensemble import RandomForestClassifier #import rf\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve, accuracy_score #metrics\n",
    "from sklearn.model_selection import GridSearchCV #grid search\n",
    "log_clf = LogisticRegression()\n",
    "svc_clf = SVC()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for fine tuning and evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_grid(classifier,model, train_file, eval_file):\n",
    "    \n",
    "    if model == 'lr': #if using logisitic regression\n",
    "        param_grid = [{'random_state':[42],\n",
    "               'C':[0.05,0.1,0.5,1],\n",
    "               'penalty':['l1','l2']}]\n",
    "        \n",
    "    if model == 'dt': #if using decision tree\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}]\n",
    "        \n",
    "    if model == 'rf': #if using random forest\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}] \n",
    "    \n",
    "    if model == 'svm': #if using svm\n",
    "        param_grid = [{'random_state':[42],\n",
    "                   'C':[0.05,0.1,1,10], \n",
    "                   'kernel':['linear','rbf']}]\n",
    "    \n",
    "    x_train, y_train, class_train = xysplit(train_file) #split training data into X, Y\n",
    "    x_eval, y_eval, class_eval = xysplit(eval_file) #split evaluation data into X, Y\n",
    "    \n",
    "    param_grid = param_grid\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, scoring='recall') #grid search using 10-folds cross validation\n",
    "    grid_search.fit(x_train, y_train) #fir grid search\n",
    "    print(\"\")\n",
    "    print('Best parameters')\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(best_parameters) #print best parameters from grid search\n",
    "    print('Best grid search score = ',grid_search.best_score_) #print best grid search score\n",
    "    print(\"\")\n",
    "    print('Evaluation data scores')\n",
    "    tuned_clf = grid_search.best_estimator_ #build model using best parameters\n",
    "    tuned_clf_pred = tuned_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "    conf_matrix = confusion_matrix(y_eval,tuned_clf_pred) #build confusion matrix\n",
    "    precision = precision_score(y_eval,tuned_clf_pred) #calculate precision\n",
    "    recall = recall_score(y_eval,tuned_clf_pred) #calculate recall\n",
    "    f1 = f1_score(y_eval,tuned_clf_pred) #calculate f1\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval,tuned_clf_pred)\n",
    "    auc_score = auc(fpr, tpr) #calculate auc\n",
    "    accuracy = accuracy_score(y_eval,tuned_clf_pred) #calculate accuracy\n",
    "    class_eval['pred'] = tuned_clf_pred\n",
    "    class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "    print(conf_matrix)\n",
    "    print('precision = ' + str(precision))\n",
    "    print('recall = ' + str(recall))\n",
    "    print('f1 = ' + str(f1))\n",
    "    print('auc = ' + str(auc_score))\n",
    "    print('accuracy = ' + str(accuracy))\n",
    "    \n",
    "    return(best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval) #return metrics and pred vs actuals for each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the \"search_grid()\" function for lr, dt, and rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.832074478607359\n",
      "\n",
      "Evaluation data scores\n",
      "[[814  17]\n",
      " [ 34 156]]\n",
      "precision = 0.9017341040462428\n",
      "recall = 0.8210526315789474\n",
      "f1 = 0.8595041322314049\n",
      "auc = 0.9002976755969345\n",
      "accuracy = 0.9500489715964741\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.832074478607359\n",
      "\n",
      "Evaluation data scores\n",
      "[[814  17]\n",
      " [ 34 156]]\n",
      "precision = 0.9017341040462428\n",
      "recall = 0.8210526315789474\n",
      "f1 = 0.8595041322314049\n",
      "auc = 0.9002976755969345\n",
      "accuracy = 0.9500489715964741\n",
      "\n",
      "Best parameters\n",
      "{'C': 1, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.8413350854386856\n",
      "\n",
      "Evaluation data scores\n",
      "[[804  27]\n",
      " [ 27 163]]\n",
      "precision = 0.8578947368421053\n",
      "recall = 0.8578947368421053\n",
      "f1 = 0.8578947368421053\n",
      "auc = 0.9127018810564317\n",
      "accuracy = 0.9471106758080313\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.8336535517813072\n",
      "\n",
      "Evaluation data scores\n",
      "[[804  27]\n",
      " [ 33 157]]\n",
      "precision = 0.8532608695652174\n",
      "recall = 0.8263157894736842\n",
      "f1 = 0.839572192513369\n",
      "auc = 0.8969124073722212\n",
      "accuracy = 0.941234084231146\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.8336535517813072\n",
      "\n",
      "Evaluation data scores\n",
      "[[804  27]\n",
      " [ 33 157]]\n",
      "precision = 0.8532608695652174\n",
      "recall = 0.8263157894736842\n",
      "f1 = 0.839572192513369\n",
      "auc = 0.8969124073722212\n",
      "accuracy = 0.941234084231146\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.1, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.864465584317715\n",
      "\n",
      "Evaluation data scores\n",
      "[[797  34]\n",
      " [ 35 155]]\n",
      "precision = 0.8201058201058201\n",
      "recall = 0.8157894736842105\n",
      "f1 = 0.8179419525065964\n",
      "auc = 0.8874374564570271\n",
      "accuracy = 0.9324191968658179\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.7396596346954539\n",
      "\n",
      "Evaluation data scores\n",
      "[[785  46]\n",
      " [ 52 138]]\n",
      "precision = 0.75\n",
      "recall = 0.7263157894736842\n",
      "f1 = 0.7379679144385027\n",
      "auc = 0.8354803977452657\n",
      "accuracy = 0.9040156709108716\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.7396596346954539\n",
      "\n",
      "Evaluation data scores\n",
      "[[785  46]\n",
      " [ 52 138]]\n",
      "precision = 0.75\n",
      "recall = 0.7263157894736842\n",
      "f1 = 0.7379679144385027\n",
      "auc = 0.8354803977452657\n",
      "accuracy = 0.9040156709108716\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.796716156577931\n",
      "\n",
      "Evaluation data scores\n",
      "[[794  37]\n",
      " [ 31 159]]\n",
      "precision = 0.8112244897959183\n",
      "recall = 0.8368421052631579\n",
      "f1 = 0.8238341968911916\n",
      "auc = 0.8961587180948762\n",
      "accuracy = 0.9333986287952988\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.6609571935228622\n",
      "\n",
      "Evaluation data scores\n",
      "[[779  52]\n",
      " [ 53 137]]\n",
      "precision = 0.7248677248677249\n",
      "recall = 0.7210526315789474\n",
      "f1 = 0.7229551451187335\n",
      "auc = 0.8292387104946483\n",
      "accuracy = 0.8971596474045054\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.6609571935228622\n",
      "\n",
      "Evaluation data scores\n",
      "[[779  52]\n",
      " [ 53 137]]\n",
      "precision = 0.7248677248677249\n",
      "recall = 0.7210526315789474\n",
      "f1 = 0.7229551451187335\n",
      "auc = 0.8292387104946483\n",
      "accuracy = 0.8971596474045054\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.7056178644096536\n",
      "\n",
      "Evaluation data scores\n",
      "[[737  94]\n",
      " [ 54 136]]\n",
      "precision = 0.591304347826087\n",
      "recall = 0.7157894736842105\n",
      "f1 = 0.6476190476190475\n",
      "auc = 0.8013363734245361\n",
      "accuracy = 0.8550440744368266\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.6147729092484786\n",
      "\n",
      "Evaluation data scores\n",
      "[[828   3]\n",
      " [ 74 116]]\n",
      "precision = 0.9747899159663865\n",
      "recall = 0.6105263157894737\n",
      "f1 = 0.7508090614886731\n",
      "auc = 0.8034581037431123\n",
      "accuracy = 0.9245837414299706\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.6147729092484786\n",
      "\n",
      "Evaluation data scores\n",
      "[[828   3]\n",
      " [ 74 116]]\n",
      "precision = 0.9747899159663865\n",
      "recall = 0.6105263157894737\n",
      "f1 = 0.7508090614886731\n",
      "auc = 0.8034581037431123\n",
      "accuracy = 0.9245837414299706\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.622496184931894\n",
      "\n",
      "Evaluation data scores\n",
      "[[825   6]\n",
      " [ 73 117]]\n",
      "precision = 0.9512195121951219\n",
      "recall = 0.6157894736842106\n",
      "f1 = 0.7476038338658147\n",
      "auc = 0.8042846285388562\n",
      "accuracy = 0.9226248775710089\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.46237606916106183\n",
      "\n",
      "Evaluation data scores\n",
      "[[826   5]\n",
      " [ 96  94]]\n",
      "precision = 0.9494949494949495\n",
      "recall = 0.49473684210526314\n",
      "f1 = 0.6505190311418685\n",
      "auc = 0.7443599974665907\n",
      "accuracy = 0.901077375122429\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'gini', 'random_state': 42}\n",
      "Best grid search score =  0.46237606916106183\n",
      "\n",
      "Evaluation data scores\n",
      "[[826   5]\n",
      " [ 96  94]]\n",
      "precision = 0.9494949494949495\n",
      "recall = 0.49473684210526314\n",
      "f1 = 0.6505190311418685\n",
      "auc = 0.7443599974665907\n",
      "accuracy = 0.901077375122429\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.43749605540797687\n",
      "\n",
      "Evaluation data scores\n",
      "[[826   5]\n",
      " [104  86]]\n",
      "precision = 0.945054945054945\n",
      "recall = 0.45263157894736844\n",
      "f1 = 0.6120996441281138\n",
      "auc = 0.7233073658876433\n",
      "accuracy = 0.8932419196865817\n",
      "time taken =  0:20:34.272899\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "current = datetime.now() #for checking duration\n",
    "\n",
    "tf = []     #initialise empty vectors to hold results\n",
    "name = []\n",
    "bp = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "tp = []\n",
    "p = []\n",
    "r = []\n",
    "f_1 = []\n",
    "auc_sc = []\n",
    "acc = []\n",
    "\n",
    "classifiers = [log_clf, dt_clf, rf_clf] #the classifiers that are to be tested\n",
    "models = ['lr','dt','rf'] #labels for identifying the results\n",
    "\n",
    "train_files = ['features/df_tweet_tfidf_train.pickle', #the file locations for the training data sets\n",
    "               'features/df_tweet_tf_train.pickle',\n",
    "               'features/df_tweet_train.pickle', \n",
    "               'features/df_tweetbio_tfidf_train.pickle',\n",
    "               'features/df_tweetbio_tf_train.pickle',\n",
    "               'features/df_tweetbio_train.pickle']\n",
    "\n",
    "eval_files = ['features/df_tweet_tfidf_eval.pickle', #the file locations for the evaluation data sets\n",
    "              'features/df_tweet_tf_eval.pickle', \n",
    "              'features/df_tweet_eval.pickle', \n",
    "              'features/df_tweetbio_tfidf_eval.pickle', \n",
    "              'features/df_tweetbio_tf_eval.pickle',\n",
    "              'features/df_tweetbio_eval.pickle']\n",
    "\n",
    "i=1 #a counter to be used for checking loop number\n",
    "for classifier, model in zip(classifiers, models): #zip through the classifiers and model names\n",
    "    for train_file, eval_file in zip(train_files, eval_files): #zip through the training and evaluation combos\n",
    "        #execute the search_grid() function\n",
    "        best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval = search_grid(classifier,model, train_file, eval_file)\n",
    "        \n",
    "        #append the latest results to the vectors\n",
    "        tf = np.append(tf,train_file)\n",
    "        name = np.append(name,model)\n",
    "        b = ';'.join('{} {}'.format(key, val) for key, val in best_parameters.items())\n",
    "        bp = np.append(bp,b)\n",
    "        tn = np.append(tn,conf_matrix[0][0])\n",
    "        fp = np.append(fp,conf_matrix[0][1])\n",
    "        fn = np.append(fn,conf_matrix[1][0])\n",
    "        tp = np.append(tp,conf_matrix[1][1])\n",
    "        p = np.append(p,precision)\n",
    "        r = np.append(r,recall)\n",
    "        f_1 = np.append(f_1,f1)\n",
    "        auc_sc = np.append(auc_sc,auc_score)\n",
    "        acc = np.append(acc,accuracy)\n",
    "        \n",
    "        #col = train_file+'_'+model #build a column name\n",
    "        #class_eval.columns = ['tweet_id',col] #rename the columns\n",
    "        class_eval['model'] = model\n",
    "        class_eval['file'] = eval_file\n",
    "        if i==1: #if we are on the first iteration of the loop\n",
    "            df = class_eval.copy()\n",
    "        else: #if we are not on the first iteration f the loop\n",
    "            #df = pd.merge(df, class_eval, on='tweet_id')\n",
    "            df = df.append(class_eval)\n",
    "        \n",
    "        i = i+1 #increment i\n",
    "\n",
    "print('time taken = ',datetime.now() - current) #print the time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the \"search_grid()\" function for svm (seperated process due to the long duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'kernel': 'linear', 'random_state': 42}\n",
      "Best grid search score =  0.8058744395146857\n",
      "\n",
      "Evaluation data scores\n",
      "[[802  29]\n",
      " [ 37 153]]\n",
      "precision = 0.8406593406593407\n",
      "recall = 0.8052631578947368\n",
      "f1 = 0.8225806451612903\n",
      "auc = 0.8851827221483312\n",
      "accuracy = 0.9353574926542605\n",
      "time taken =  0:19:11.994151\n"
     ]
    }
   ],
   "source": [
    "current = datetime.now()\n",
    "\n",
    "classifiers = [svc_clf] #the classifier that is to be tested\n",
    "models = ['svm'] #a lebale used to identify the results\n",
    "\n",
    "train_files = ['features/df_tweet_train.pickle'] #the location of the training data\n",
    "\n",
    "eval_files = ['features/df_tweet_eval.pickle'] #the location of the evaluation data\n",
    "\n",
    "for classifier, model in zip(classifiers, models): #zip through the classifier and label combos\n",
    "    for train_file, eval_file in zip(train_files, eval_files): #zip through the train and evaluation data combos\n",
    "        #execute the search_grid() function\n",
    "        best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval = search_grid(classifier,model, train_file, eval_file)\n",
    "        \n",
    "        #append the latest results to the vectors\n",
    "        tf = np.append(tf,train_file)\n",
    "        name = np.append(name,model)\n",
    "        b = ';'.join('{} {}'.format(key, val) for key, val in best_parameters.items())\n",
    "        bp = np.append(bp,b)\n",
    "        tn = np.append(tn,conf_matrix[0][0])\n",
    "        fp = np.append(fp,conf_matrix[0][1])\n",
    "        fn = np.append(fn,conf_matrix[1][0])\n",
    "        tp = np.append(tp,conf_matrix[1][1])\n",
    "        p = np.append(p,precision)\n",
    "        r = np.append(r,recall)\n",
    "        f_1 = np.append(f_1,f1)\n",
    "        auc_sc = np.append(auc_sc,auc_score)\n",
    "        acc = np.append(acc,accuracy)\n",
    "        \n",
    "        class_eval['model'] = model\n",
    "        class_eval['file'] = eval_file\n",
    "\n",
    "        df = df.append(class_eval) #merge the latest predictions for each tweet using this classifier\n",
    "\n",
    "\n",
    "print('time taken = ',datetime.now() - current) #print the time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble classifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[820  11]\n",
      " [ 38 152]]\n",
      "precision = 0.9325153374233128\n",
      "recall = 0.8\n",
      "f1 = 0.8611898016997167\n",
      "auc = 0.89338146811071\n",
      "accuracy = 0.9520078354554359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(penalty='l2',C=1, random_state=42) #logistic regression with best hyperparameters\n",
    "svc_clf = SVC(C=0.05, kernel='linear', probability = True, random_state=42) #svm with best hyperparameters\n",
    "rf_clf = DecisionTreeClassifier(criterion='gini', random_state=42) #random forest with best hyperparameters\n",
    "dt_clf = RandomForestClassifier(criterion='gini', random_state=42) #random forest with best hyperparameters\n",
    "\n",
    "#create the ensemble\n",
    "e_clf = VotingClassifier(estimators=[('lr', log_clf), ('svm', svc_clf), ('rf', rf_clf), ('dt', dt_clf)],\n",
    "                         voting='soft', weights=[1, 1, 1, 1])\n",
    "\n",
    "#get training and evaluation data\n",
    "x_train, y_train, class_train = xysplit('features/df_tweet_train.pickle') #split training data into X, Y\n",
    "x_eval, y_eval, class_eval = xysplit('features/df_tweet_eval.pickle') #split evaluation data into X, Y\n",
    "\n",
    "e_clf = e_clf.fit(x_train, y_train) #fit the ensemble\n",
    "\n",
    "e_clf_pred = e_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "conf_matrix = confusion_matrix(y_eval,e_clf_pred) #build confusion matrix\n",
    "precision = precision_score(y_eval,e_clf_pred) #calculate precision\n",
    "recall = recall_score(y_eval,e_clf_pred) #calculate recall\n",
    "f1 = f1_score(y_eval,e_clf_pred) #calculate f1\n",
    "fpr, tpr, thresholds = roc_curve(y_eval,e_clf_pred)\n",
    "auc_score = auc(fpr, tpr) #calculate auc\n",
    "accuracy = accuracy_score(y_eval,e_clf_pred) #calculate accuracy\n",
    "class_eval['pred'] = e_clf_pred\n",
    "class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "print(conf_matrix)\n",
    "print('precision = ' + str(precision))\n",
    "print('recall = ' + str(recall))\n",
    "print('f1 = ' + str(f1))\n",
    "print('auc = ' + str(auc_score))\n",
    "print('accuracy = ' + str(accuracy))\n",
    "\n",
    "#append the latest results to the vectors\n",
    "tf = np.append(tf,'features/df_tweet_train.pickle')\n",
    "name = np.append(name,'ensemble')\n",
    "bp = np.append(bp,'ensemble')\n",
    "tn = np.append(tn,conf_matrix[0][0])\n",
    "fp = np.append(fp,conf_matrix[0][1])\n",
    "fn = np.append(fn,conf_matrix[1][0])\n",
    "tp = np.append(tp,conf_matrix[1][1])\n",
    "p = np.append(p,precision)\n",
    "r = np.append(r,recall)\n",
    "f_1 = np.append(f_1,f1)\n",
    "auc_sc = np.append(auc_sc,auc_score)\n",
    "acc = np.append(acc,accuracy)\n",
    "\n",
    "class_eval['model'] = 'ensemble'\n",
    "class_eval['file'] = 'features/df_tweet_eval.pickle'\n",
    "\n",
    "df = df.append(class_eval) #merge the latest predictions for each tweet using this classifier\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('results/classifier_results.pickle') #pickle the results of actual vs predicted for each tweet\n",
    "\n",
    "classifications = pd.DataFrame({'tf':tf, #create a dataframe to hold the metrics\n",
    "                                'name':name,\n",
    "                               'bp':bp,\n",
    "                               'tn':tn,\n",
    "                               'fp':fp,\n",
    "                               'fn':fn,\n",
    "                               'tp':tp,\n",
    "                               'p':p,\n",
    "                               'r':r,\n",
    "                               'f_1':f_1,\n",
    "                               'auc_sc':auc_sc,\n",
    "                               'acc':acc})\n",
    "classifications.to_pickle('results/classifications.pickle') #pickle the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print metric results descending by f1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>auc_sc</th>\n",
       "      <th>bp</th>\n",
       "      <th>f_1</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>name</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>tf</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.952008</td>\n",
       "      <td>0.893381</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.861190</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.932515</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>features/df_tweet_train.pickle</td>\n",
       "      <td>820.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950049</td>\n",
       "      <td>0.900298</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>34.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>features/df_tweet_tf_train.pickle</td>\n",
       "      <td>814.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950049</td>\n",
       "      <td>0.900298</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>34.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>features/df_tweet_tfidf_train.pickle</td>\n",
       "      <td>814.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.947111</td>\n",
       "      <td>0.912702</td>\n",
       "      <td>C 1;penalty l2;random_state 42</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>features/df_tweet_train.pickle</td>\n",
       "      <td>804.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941234</td>\n",
       "      <td>0.896912</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.853261</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>features/df_tweetbio_tfidf_train.pickle</td>\n",
       "      <td>804.0</td>\n",
       "      <td>157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941234</td>\n",
       "      <td>0.896912</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.853261</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>features/df_tweetbio_tf_train.pickle</td>\n",
       "      <td>804.0</td>\n",
       "      <td>157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.933399</td>\n",
       "      <td>0.896159</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>features/df_tweet_train.pickle</td>\n",
       "      <td>794.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.935357</td>\n",
       "      <td>0.885183</td>\n",
       "      <td>C 0.05;kernel linear;random_state 42</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.840659</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>features/df_tweet_train.pickle</td>\n",
       "      <td>802.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.932419</td>\n",
       "      <td>0.887437</td>\n",
       "      <td>C 0.1;penalty l2;random_state 42</td>\n",
       "      <td>0.817942</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.820106</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>features/df_tweetbio_train.pickle</td>\n",
       "      <td>797.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.924584</td>\n",
       "      <td>0.803458</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.750809</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>features/df_tweet_tfidf_train.pickle</td>\n",
       "      <td>828.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.924584</td>\n",
       "      <td>0.803458</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.750809</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>features/df_tweet_tf_train.pickle</td>\n",
       "      <td>828.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.922625</td>\n",
       "      <td>0.804285</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.747604</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>features/df_tweet_train.pickle</td>\n",
       "      <td>825.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.904016</td>\n",
       "      <td>0.835480</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>52.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>features/df_tweet_tfidf_train.pickle</td>\n",
       "      <td>785.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.904016</td>\n",
       "      <td>0.835480</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>52.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>features/df_tweet_tf_train.pickle</td>\n",
       "      <td>785.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.897160</td>\n",
       "      <td>0.829239</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.721053</td>\n",
       "      <td>features/df_tweetbio_tf_train.pickle</td>\n",
       "      <td>779.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.897160</td>\n",
       "      <td>0.829239</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.721053</td>\n",
       "      <td>features/df_tweetbio_tfidf_train.pickle</td>\n",
       "      <td>779.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.901077</td>\n",
       "      <td>0.744360</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.650519</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>features/df_tweetbio_tfidf_train.pickle</td>\n",
       "      <td>826.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.901077</td>\n",
       "      <td>0.744360</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.650519</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>features/df_tweetbio_tf_train.pickle</td>\n",
       "      <td>826.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.855044</td>\n",
       "      <td>0.801336</td>\n",
       "      <td>criterion gini;random_state 42</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>54.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>features/df_tweetbio_train.pickle</td>\n",
       "      <td>737.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.893242</td>\n",
       "      <td>0.723307</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>features/df_tweetbio_train.pickle</td>\n",
       "      <td>826.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc    auc_sc                                    bp       f_1     fn  \\\n",
       "19  0.952008  0.893381                              ensemble  0.861190   38.0   \n",
       "1   0.950049  0.900298     C 0.05;penalty l2;random_state 42  0.859504   34.0   \n",
       "0   0.950049  0.900298     C 0.05;penalty l2;random_state 42  0.859504   34.0   \n",
       "2   0.947111  0.912702        C 1;penalty l2;random_state 42  0.857895   27.0   \n",
       "3   0.941234  0.896912     C 0.05;penalty l2;random_state 42  0.839572   33.0   \n",
       "4   0.941234  0.896912     C 0.05;penalty l2;random_state 42  0.839572   33.0   \n",
       "8   0.933399  0.896159        criterion gini;random_state 42  0.823834   31.0   \n",
       "18  0.935357  0.885183  C 0.05;kernel linear;random_state 42  0.822581   37.0   \n",
       "5   0.932419  0.887437      C 0.1;penalty l2;random_state 42  0.817942   35.0   \n",
       "12  0.924584  0.803458        criterion gini;random_state 42  0.750809   74.0   \n",
       "13  0.924584  0.803458        criterion gini;random_state 42  0.750809   74.0   \n",
       "14  0.922625  0.804285        criterion gini;random_state 42  0.747604   73.0   \n",
       "6   0.904016  0.835480     criterion entropy;random_state 42  0.737968   52.0   \n",
       "7   0.904016  0.835480     criterion entropy;random_state 42  0.737968   52.0   \n",
       "10  0.897160  0.829239        criterion gini;random_state 42  0.722955   53.0   \n",
       "9   0.897160  0.829239        criterion gini;random_state 42  0.722955   53.0   \n",
       "15  0.901077  0.744360        criterion gini;random_state 42  0.650519   96.0   \n",
       "16  0.901077  0.744360        criterion gini;random_state 42  0.650519   96.0   \n",
       "11  0.855044  0.801336        criterion gini;random_state 42  0.647619   54.0   \n",
       "17  0.893242  0.723307     criterion entropy;random_state 42  0.612100  104.0   \n",
       "\n",
       "      fp      name         p         r  \\\n",
       "19  11.0  ensemble  0.932515  0.800000   \n",
       "1   17.0        lr  0.901734  0.821053   \n",
       "0   17.0        lr  0.901734  0.821053   \n",
       "2   27.0        lr  0.857895  0.857895   \n",
       "3   27.0        lr  0.853261  0.826316   \n",
       "4   27.0        lr  0.853261  0.826316   \n",
       "8   37.0        dt  0.811224  0.836842   \n",
       "18  29.0       svm  0.840659  0.805263   \n",
       "5   34.0        lr  0.820106  0.815789   \n",
       "12   3.0        rf  0.974790  0.610526   \n",
       "13   3.0        rf  0.974790  0.610526   \n",
       "14   6.0        rf  0.951220  0.615789   \n",
       "6   46.0        dt  0.750000  0.726316   \n",
       "7   46.0        dt  0.750000  0.726316   \n",
       "10  52.0        dt  0.724868  0.721053   \n",
       "9   52.0        dt  0.724868  0.721053   \n",
       "15   5.0        rf  0.949495  0.494737   \n",
       "16   5.0        rf  0.949495  0.494737   \n",
       "11  94.0        dt  0.591304  0.715789   \n",
       "17   5.0        rf  0.945055  0.452632   \n",
       "\n",
       "                                         tf     tn     tp  \n",
       "19           features/df_tweet_train.pickle  820.0  152.0  \n",
       "1         features/df_tweet_tf_train.pickle  814.0  156.0  \n",
       "0      features/df_tweet_tfidf_train.pickle  814.0  156.0  \n",
       "2            features/df_tweet_train.pickle  804.0  163.0  \n",
       "3   features/df_tweetbio_tfidf_train.pickle  804.0  157.0  \n",
       "4      features/df_tweetbio_tf_train.pickle  804.0  157.0  \n",
       "8            features/df_tweet_train.pickle  794.0  159.0  \n",
       "18           features/df_tweet_train.pickle  802.0  153.0  \n",
       "5         features/df_tweetbio_train.pickle  797.0  155.0  \n",
       "12     features/df_tweet_tfidf_train.pickle  828.0  116.0  \n",
       "13        features/df_tweet_tf_train.pickle  828.0  116.0  \n",
       "14           features/df_tweet_train.pickle  825.0  117.0  \n",
       "6      features/df_tweet_tfidf_train.pickle  785.0  138.0  \n",
       "7         features/df_tweet_tf_train.pickle  785.0  138.0  \n",
       "10     features/df_tweetbio_tf_train.pickle  779.0  137.0  \n",
       "9   features/df_tweetbio_tfidf_train.pickle  779.0  137.0  \n",
       "15  features/df_tweetbio_tfidf_train.pickle  826.0   94.0  \n",
       "16     features/df_tweetbio_tf_train.pickle  826.0   94.0  \n",
       "11        features/df_tweetbio_train.pickle  737.0  136.0  \n",
       "17        features/df_tweetbio_train.pickle  826.0   86.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications.sort_values(by='f_1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
