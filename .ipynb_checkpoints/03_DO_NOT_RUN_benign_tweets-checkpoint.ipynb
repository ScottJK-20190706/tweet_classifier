{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter code adapted from http://socialmedia-class.org/twittertutorial.html\n",
    "#Twitter api docs at https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter api tokens and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '<api key goes here>'\n",
    "api_secret_key = '<api secret key goes here>'\n",
    "access_token = '<access token goes here>'\n",
    "access_token_secret = '<access token secret goes here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "\n",
    "#auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "# wait_on_rate_limit= True;  will make the api to automatically wait for rate limits to replenish\n",
    "# wait_on_rate_limit_notify= Ture;  will make the api  to print a notification when Tweepyis waiting for rate limits to replenish\n",
    "#---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(listOfTweets, keyword, numOfTweets,start_id,max_id,update):\n",
    "    # Iterate through all tweets containing the given word, api search mode\n",
    "    if update==1:\n",
    "        tweets = tweepy.Cursor(api.search, q=keyword, tweet_mode='extended', since_id=str(start_id+1),max_id=str(max_id-1)).items(numOfTweets)\n",
    "    if update==0:\n",
    "        tweets = tweepy.Cursor(api.search, q=keyword, tweet_mode='extended', max_id=str(start_id-1)).items(numOfTweets)\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            #get results in json format\n",
    "            status = tweet\n",
    "            json_str = json.dumps(status._json)\n",
    "            json_str = json.loads(json_str)\n",
    "            \n",
    "            #get the necessary values from each tweet\n",
    "            id = json_str['id']\n",
    "            screen_name = json_str['user']['screen_name']\n",
    "            user_name = json_str['user']['name']\n",
    "            followers = json_str['user']['followers_count']\n",
    "            following = json_str['user']['friends_count']\n",
    "            tweet_created_at = json_str['created_at']\n",
    "            tweet_text = json_str['full_text']\n",
    "            bio = json_str['user']['description']\n",
    "            location = json_str['user']['location']\n",
    "            #coord = json_str['coordinates']['coordinates']\n",
    "            coord = ''\n",
    "            no_tweets = json_str['user']['statuses_count']\n",
    "\n",
    "            if len(json_str['entities']['hashtags'])==0:\n",
    "                hashtags = ''\n",
    "            else:\n",
    "                hashtags = []\n",
    "                for i in range(0,len(json_str['entities']['hashtags'])):\n",
    "                    hashtags = pd.Series(np.append(hashtags,json_str['entities']['hashtags'][i]['text']))\n",
    "                    hashtags = hashtags.str.cat(sep=' ')\n",
    "\n",
    "            if len(json_str['entities']['urls'])==0:\n",
    "                urls=''\n",
    "            else:\n",
    "                urls = []\n",
    "                for i in range(0,len(json_str['entities']['urls'])):\n",
    "                    urls = pd.Series(np.append(urls,json_str['entities']['urls'][i]['url']))\n",
    "                    urls = urls.str.cat(sep=' ')\n",
    "\n",
    "            if len(json_str['entities']['user_mentions'])==0:\n",
    "                mentions = ''\n",
    "            else:\n",
    "                mentions = []\n",
    "                for i in range(0,len(json_str['entities']['user_mentions'])):\n",
    "                    mentions = pd.Series(np.append(mentions,json_str['entities']['user_mentions'][i]['screen_name']))\n",
    "                    mentions = mentions.str.cat(sep=' ')\n",
    "\n",
    "            # Add tweets in this format\n",
    "            dict_ = {'Tweet ID': id,\n",
    "                    'Name': screen_name,\n",
    "                    'Username': user_name,\n",
    "                    'Number of Followers': followers,\n",
    "                    'Number Following': following,\n",
    "                    'Time': tweet_created_at,\n",
    "                    'Tweet': tweet_text,\n",
    "                     'Hashtags': hashtags,\n",
    "                     'Web': urls,\n",
    "                     'Bio': bio,\n",
    "                     'Location': location,\n",
    "                     'Location Coordinates': coord,\n",
    "                     'Number of Tweets': no_tweets,\n",
    "                     'Mentions': mentions\n",
    "                    }\n",
    "            listOfTweets.append(dict_)\n",
    "        except:\n",
    "            pass \n",
    "    return listOfTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "listOfTweets=[] #empty array for holding tweets\n",
    "#search terms\n",
    "keyword = '(nicola sturgeon) OR (snp sturgeon)  OR (scotland sturgeon) OR (scottish sturgeon) OR (holyrood sturgeon)  OR (westminster sturgeon) -filter:retweets'\n",
    "#no of tweets per page\n",
    "numOfTweets = 100\n",
    "\n",
    "try:\n",
    "    existing_data = pd.read_pickle('pickle_files/existing_data.pickle') #if database of tweets already exists\n",
    "    existing_data.to_pickle('pickle_files/existing_data_backup.pickle')\n",
    "    print(\"success\")\n",
    "    since_id = max(existing_data['Tweet ID']) #find the max tweetd id from existing database\n",
    "    max_id = 9999999999999999999\n",
    "    for i in range(0,30):\n",
    "        x = get_tweets(listOfTweets, keyword, numOfTweets, since_id,max_id,1) #get tweets with tweetd id > since_id\n",
    "        df = pd.DataFrame(x) #convert to dataframe\n",
    "        max_id = min(df['Tweet ID']) #reset since_id\n",
    "    existing_data = existing_data.append(df) #append new results to exisiting results\n",
    "    existing_data = existing_data.reset_index(drop=True) #reset the index\n",
    "    existing_data = existing_data.drop_duplicates() #just in case!\n",
    "    existing_data.to_pickle('pickle_files/existing_data.pickle') #save the dataframe\n",
    "except: #database of tweets does not yet exist\n",
    "    pass\n",
    "    #print(\"failure\")\n",
    "    #max_id=1150486892451946496 #this is the start point for this exercise\n",
    "    #for i in range(0,30):\n",
    "    #    x = get_tweets(listOfTweets, keyword, numOfTweets, max_id,0) #get tweets with tweet id < max_id\n",
    "    #    df = pd.DataFrame(x) #convert to dataframe\n",
    "    #    max_id = min(df['Tweet ID']) #reset max_id\n",
    "    #df.to_pickle('pickle_files/existing_data.pickle') #save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14804, 14)\n"
     ]
    }
   ],
   "source": [
    "existing_data = pd.read_pickle('pickle_files/existing_data.pickle')\n",
    "print(existing_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all tweets that contain a term from hatebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets = [] #to hold list of ids containing 'hatebase' terms\n",
    "counter = 1\n",
    "hate_terms = pd.read_pickle('pickle_files/hate_terms.pickle') #read in the terms from hatebase\n",
    "hate_lower = hate_terms.hate_terms.str.lower() #lowercase all hate terms\n",
    "existing_data_lower = pd.read_pickle('pickle_files/existing_data.pickle') #read in all the Nicola Sturgeon tweets\n",
    "existing_data_lower.Tweet = existing_data_lower.Tweet.str.lower() #lowercase the Nicola Sturgeon tweets\n",
    "chars = [' ','!','.'] #additional leading and trailing characters for the hatebase terms\n",
    "for t in hate_lower: #loop through each hatebase term\n",
    "    for lead in chars:\n",
    "        for trail in chars:\n",
    "            try:\n",
    "                term = lead + t + trail #add the lead and trail characters\n",
    "                hate_twitter = existing_data_lower[existing_data_lower.Tweet.str.contains(term)]\n",
    "                hate_tweets = np.append(hate_tweets,hate_twitter.index) #add the index if the tweet contains a hatebase term\n",
    "            except:\n",
    "                pass\n",
    "hate_tweets = np.unique(hate_tweets) #remove the duplicates\n",
    "existing_clean = pd.read_pickle('pickle_files/existing_data.pickle')\n",
    "existing_clean = existing_clean[~existing_clean.index.isin(hate_tweets)] #drop all the Nicola Sturgeon tweets that include a hatebase term\n",
    "\n",
    "existing_clean.to_pickle('pickle_files/existing_clean.pickle') #save the dataframe of \"clean\" tweets\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5706, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
