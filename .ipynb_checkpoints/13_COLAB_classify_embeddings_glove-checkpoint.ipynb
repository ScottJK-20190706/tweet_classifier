{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK2Kc3gt2fde"
   },
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3602,
     "status": "ok",
     "timestamp": 1571137106881,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "-w6veqQd5x0_",
    "outputId": "997ce12b-f87f-4136-8e68-8a981a1d3d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5344,
     "status": "ok",
     "timestamp": 1571137113914,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "ZtF8MLUOUhKm",
    "outputId": "5b1a065e-dda6-4e32-844b-3c87d1d64377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcsfs\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/92/0297f2813cb240c52e90f8587420149970565800e019e1b08ef5ad28b6d9/gcsfs-0.3.1.tar.gz (43kB)\n",
      "\r",
      "\u001b[K     |███████▋                        | 10kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 20kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 30kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 40kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 2.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.5.1)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.6)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
      "Building wheels for collected packages: gcsfs\n",
      "  Building wheel for gcsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gcsfs: filename=gcsfs-0.3.1-py2.py3-none-any.whl size=17936 sha256=d58606cb5fec694f2644eb212c23ffba3820c260048f243cd6d22257515ccc64\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/2b/6f/86954f0d8caa1173841e62bb780dc0f8693bd268e04a267682\n",
      "Successfully built gcsfs\n",
      "Installing collected packages: gcsfs\n",
      "Successfully installed gcsfs-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gcsfs #google cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3303,
     "status": "ok",
     "timestamp": 1571137119741,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "YQlC6GQZU2g2",
    "outputId": "828fd079-0ab9-4da1-d720-23b5a6f6932d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chakin\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/3f/ca2f63451c0ab47970a6ab1d39d96118e70b6e73125529cea767c31368a3/chakin-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: progressbar2>=3.20.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (3.38.0)\n",
      "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python3.6/dist-packages (from chakin) (0.24.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (1.12.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2>=3.20.0->chakin) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2.5.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (1.16.5)\n",
      "Installing collected packages: chakin\n",
      "Successfully installed chakin-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install chakin #word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_l6H1bi2uA4"
   },
   "source": [
    "## Obtain zipped word embeddings from chakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1014759,
     "status": "ok",
     "timestamp": 1571138136385,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "ClG_IAx3w-Zv",
    "outputId": "17651c58-1a98-41ef-d5c7-0124ba53072c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension  ... Language    Author\n",
      "2          fastText(en)        300  ...  English  Facebook\n",
      "11         GloVe.6B.50d         50  ...  English  Stanford\n",
      "12        GloVe.6B.100d        100  ...  English  Stanford\n",
      "13        GloVe.6B.200d        200  ...  English  Stanford\n",
      "14        GloVe.6B.300d        300  ...  English  Stanford\n",
      "15       GloVe.42B.300d        300  ...  English  Stanford\n",
      "16      GloVe.840B.300d        300  ...  English  Stanford\n",
      "17    GloVe.Twitter.25d         25  ...  English  Stanford\n",
      "18    GloVe.Twitter.50d         50  ...  English  Stanford\n",
      "19   GloVe.Twitter.100d        100  ...  English  Stanford\n",
      "20   GloVe.Twitter.200d        200  ...  English  Stanford\n",
      "21  word2vec.GoogleNews        300  ...  English    Google\n",
      "\n",
      "[12 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:16:53   2.0 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/glove.840B.300d.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chakin\n",
    "chakin.search(lang='English')\n",
    "chakin.download(number=16, save_dir='/tmp/') # select GloVe.840B.300d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BULlgu0S22ct"
   },
   "source": [
    "## Unzip the word embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jg8LxKZyxsV9"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('/tmp/glove.840B.300d.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "  zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nw36c_5d3Bfh"
   },
   "source": [
    "\n",
    "## Function for loading the word embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VeoAeudyzIE"
   },
   "outputs": [],
   "source": [
    "#code adapted from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8N9rcLh4rEV"
   },
   "source": [
    "## Load the word embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186071,
     "status": "ok",
     "timestamp": 1571138767487,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "80eHRiO34wyq",
    "outputId": "ef83bf74-0f9e-4a3f-f1a8-fcaeff4db1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 2196016  words loaded!\n"
     ]
    }
   ],
   "source": [
    "w = loadGloveModel('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN4o6BHo3J_X"
   },
   "source": [
    "## Upload the Tweets pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7C42hdJQz9O1"
   },
   "outputs": [],
   "source": [
    "! wget -cq https://github.com/ScottJK-20190706/tweet_classifier/blob/master/pickle_files//train_data_formatted.pickle?raw=true\n",
    "! wget -cq https://github.com/ScottJK-20190706/tweet_classifier/blob/master/pickle_files/eval_data_formatted.pickle?raw=true\n",
    "! wget -cq https://github.com/ScottJK-20190706/tweet_classifier/blob/master/features/count_features/count_features.pickle?raw=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jY-UpqMn3QUQ"
   },
   "source": [
    "## Function for transforming tweets to word embedding arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZH6rto75Yl8"
   },
   "outputs": [],
   "source": [
    "def embedding(df,length):\n",
    "    tokens = df.loc[:,['Tweet ID','Tweet','class']] #slice the text and tweet id\n",
    "    p = re.compile(r'[^\\w\\s]+')\n",
    "    tokens['Tweet'] = [p.sub('', x) for x in tokens['Tweet'].tolist()] #remove the punctuation\n",
    "    tokens['Tweet'] = tokens['Tweet'].apply(word_tokenize) #tokenize the text\n",
    "    tokens = tokens.reset_index()\n",
    "\n",
    "    seq_sizes = [] #initialize a list to hold length in tokens of each tweet\n",
    "    for i in tokens.index:\n",
    "        seq_sizes = np.append(seq_sizes, len(tokens.Tweet[i])) #get all the tweet token lengths\n",
    "    max_len = int(round(np.percentile(seq_sizes,100))) #find the number of tokens equal to the 100th percentile of token lengths\n",
    "    #max_len = max_len//4*4 #find the nearest number divisible by four\n",
    "    print(max_len)\n",
    "\n",
    "    vector_size = length #this is the length of the pretrained word embeddings\n",
    "    vec_array = np.zeros(tokens.shape[0]*max_len*vector_size) #initialize an array to hold enbeddings\n",
    "    #vec_array = np.zeros(1000*max_len*vector_size) #initialize an array to hold enbeddings\n",
    "    id_array = [] #initialize an array for the tweetd ids\n",
    "    labels = [] #initialize an array for the labels\n",
    "    missing_tokens = [] #initialize an array to hold missing tokens\n",
    "    found_tokens = [] #initialize an array to hold missing tokens\n",
    "    for i in range(0,tokens.shape[0]): #loop through each tweet\n",
    "    #for i in range(0,1000): #loop through each tweet\n",
    "        id_array = np.append(id_array,tokens['Tweet ID'][i])\n",
    "        labels = np.append(labels,tokens['class'][i])\n",
    "        if i%500==0:\n",
    "            print(i) #print progress\n",
    "        for j in range(0,max_len): #loop through each token\n",
    "            try:\n",
    "                w2v = w[tokens.Tweet[i][j]] #check if the token has a pretrained embedding\n",
    "                found_tokens = np.append(found_tokens,tokens.Tweet[i][j])\n",
    "                for k in range(0,vector_size):\n",
    "                    position = (i*max_len*vector_size)+(j*vector_size)+k\n",
    "                    vec_array[position] = w2v[k] #loop through each element of the token's embedding and add to vec_array\n",
    "            except:\n",
    "                try:\n",
    "                    missing_tokens = np.append(missing_tokens,tokens.Tweet[i][j])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    vec_array = np.reshape(vec_array,(tokens.shape[0],max_len,vector_size)) #reshape vec_array\n",
    "    missing_tokens = np.unique(missing_tokens)\n",
    "    found_tokens = np.unique(found_tokens)\n",
    "    \n",
    "    return (vec_array, id_array, labels, max_len, missing_tokens, found_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCHg4XQ93aSo"
   },
   "source": [
    "## Transform the tweets to word embedding arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 104536,
     "status": "ok",
     "timestamp": 1571139324326,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "K5VlPBQT2NHF",
    "outputId": "56c33067-9c96-4cc3-e86e-762d52735cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "55\n",
      "0\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "vec_len = 300\n",
    "import pickle\n",
    "import pandas as pd\n",
    "train_data = pd.read_pickle('train_data_formatted.pickle?raw=true')\n",
    "eval_data = pd.read_pickle('eval_data_formatted.pickle?raw=true')\n",
    "train_data, train_id, train_labels, train_max_len, train_missing_tokens, train_found_tokens = embedding(train_data,vec_len)\n",
    "eval_data, eval_id, eval_labels, eval_max_len, eval_missing_tokens, eval_found_tokens = embedding(eval_data,vec_len)\n",
    "train_labels = train_labels.astype(int)\n",
    "eval_labels = eval_labels.astype(int)\n",
    "del w #clear some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1jhqXXRZG-o"
   },
   "source": [
    "## Authenticate location for saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26571,
     "status": "ok",
     "timestamp": 1571139354852,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "-CJWumDaUHLX",
    "outputId": "3589d63f-3e75-42fb-c8dd-338ac0c739d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "***** File output directory: gs://dissertation_bucket/classify_embeddings_glove *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "FILE_OUTPUT_DIR = 'classify_embeddings_glove'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = True #@param {type:\"boolean\"}\n",
    "BUCKET = 'dissertation_bucket' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  FILE_OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, FILE_OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(FILE_OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(FILE_OUTPUT_DIR)\n",
    "print('***** File output directory: {} *****'.format(FILE_OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSi_JGRZZeYO"
   },
   "source": [
    "## Save file detailing tokens that were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHQCB8ikWvK2"
   },
   "outputs": [],
   "source": [
    "max_len = max(train_max_len, eval_max_len)\n",
    "found_tokens = np.unique(np.append(train_found_tokens,eval_found_tokens))\n",
    "missing_tokens = np.unique(np.append(train_missing_tokens,eval_missing_tokens))\n",
    "found = pd.DataFrame({'token': found_tokens})\n",
    "found['found'] = 1\n",
    "missing = pd.DataFrame({'token': missing_tokens})\n",
    "missing['found'] = 0\n",
    "tokens = found.append(missing)\n",
    "filename = FILE_OUTPUT_DIR + '/tokens.csv'\n",
    "tokens.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdkSBNOqC-K3"
   },
   "source": [
    "## Function for summing embeddings over each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3SpJlSPVJOT"
   },
   "outputs": [],
   "source": [
    "def aggregate_vectors(data,max_len,tweet_id):\n",
    "  count_features = pd.read_pickle('count_features.pickle?raw=true')\n",
    "  count_features_list = ['ave_chars_token', 'caps_count', 'followers_count', 'following_count',\n",
    "                         'mention_count', 'neg_sent', 'neu_sent', 'pos_sent','posted_tweets_count',\n",
    "                         'punctuation_count', 'quotes_count', 'url_count'] \n",
    "  embed_vec_sum = np.zeros(data.shape[0]*vec_len) #initialize an array to hold sum of embeddings\n",
    "\n",
    "  embed_vec_sum = np.reshape(embed_vec_sum,(data.shape[0],vec_len)) #reshape embed_vec_sum\n",
    "\n",
    "  for i in range(0,data.shape[0]): #loop through the arrays\n",
    "    token_count = max_len #there are max_len potential tokens in each array\n",
    "    if np.sum(data[i])==0: #some tweets might have vectors that are all zeros\n",
    "      embed_vec_sum[i] = np.zeros(vec_len) #set vector to zeros\n",
    "    else:\n",
    "      if i%500==0:\n",
    "        print(i) #print progress\n",
    "      for j in range(0,max_len):\n",
    "        if np.sum(data[i][j])==0: #check for tokens that are currently zeros\n",
    "          token_count = token_count-1 #reduce token count by 1 when token is all zeros\n",
    "      embed_vec_sum[i] = np.add.reduce(data[i]) #add each of the individual word embeddings in the tweet\n",
    "\n",
    "  \n",
    "  \n",
    "  #*******************create arrays including count features**********************************************\n",
    "  embed_vec_count_sum = np.zeros(data.shape[0]*(vec_len+len(count_features_list))) #initialize an array to hold sum of embeddings inc count features\n",
    "\n",
    "  embed_vec_count_sum = np.reshape(embed_vec_count_sum,(data.shape[0],(vec_len+len(count_features_list)))) #reshape embed_vec_count_sum\n",
    "  \n",
    "  for i in range(0,data.shape[0]): #loop through each tweet\n",
    "    add_count = count_features[(count_features.tweet_ids==tweet_id[i])&(count_features.feature.isin(count_features_list))]['value'].values #count features for each tweet\n",
    "    if len(add_count)==0: #if there are no count features for tweet\n",
    "      embed_vec_count_sum[i] = np.append(embed_vec_sum[i],np.zeros(len(count_features_list))) #add zeros\n",
    "\n",
    "    else:\n",
    "      embed_vec_count_sum[i] = np.append(embed_vec_sum[i],add_count) #append the count features to the embedding features\n",
    "    \n",
    "      \n",
    "      \n",
    "  return(embed_vec_sum,embed_vec_count_sum)\n",
    "  #return(embed_vec_sum,embed_vec_ave)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHAW02yIDGRk"
   },
   "source": [
    "## Sum embeddings over each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50808,
     "status": "ok",
     "timestamp": 1571139418329,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "GxJnOgMtdpmC",
    "outputId": "0009f37e-4860-4cb4-f8e4-e57b9c8cc09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "0\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_vec_sum, train_vec_count_sum = aggregate_vectors(train_data,train_max_len,train_id)\n",
    "eval_vec_sum, eval_vec_count_sum = aggregate_vectors(eval_data,eval_max_len,eval_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPIK4Bx1DNnK"
   },
   "source": [
    "## Import classifiers and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wM4e9z9XeESK"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #import lr\n",
    "from sklearn.svm import SVC #import svm\n",
    "from sklearn.tree import DecisionTreeClassifier #import dt\n",
    "from sklearn.ensemble import RandomForestClassifier #import rf\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve, accuracy_score #metrics\n",
    "from sklearn.model_selection import GridSearchCV #grid search\n",
    "log_clf = LogisticRegression()\n",
    "svc_clf = SVC()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phlpM8vcDXbR"
   },
   "source": [
    "## Grid search and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRfPwYo3QEcp"
   },
   "outputs": [],
   "source": [
    "def search_grid(classifier, model, x_train, y_train, class_train, x_eval, y_eval, class_eval):\n",
    "    \n",
    "    if model == 'lr': #if using logisitic regression\n",
    "        param_grid = [{'random_state':[42],\n",
    "               'C':[0.05,0.1,0.5,1],\n",
    "               'penalty':['l1','l2']}]\n",
    "        \n",
    "    if model == 'dt': #if using decision tree\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}]\n",
    "        \n",
    "    if model == 'rf': #if using random forest\n",
    "        param_grid = [{'random_state':[42],\n",
    "                       'criterion':['gini','entropy']}] \n",
    "    \n",
    "    if model == 'svm': #if using svm\n",
    "        param_grid = [{'random_state':[42],\n",
    "                   'C':[0.05,0.1,1,10], \n",
    "                   'kernel':['linear','rbf']}]\n",
    "    \n",
    "  \n",
    "    param_grid = param_grid\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, scoring='recall') #grid search using 10-folds cross validation\n",
    "    grid_search.fit(x_train, y_train) #fir grid search\n",
    "    print(\"\")\n",
    "    print('Best parameters')\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(best_parameters) #print best parameters from grid search\n",
    "    print('Best grid search score = ',grid_search.best_score_) #print best grid search score\n",
    "    print(\"\")\n",
    "    print('Evaluation data scores')\n",
    "    tuned_clf = grid_search.best_estimator_ #build model using best parameters\n",
    "    tuned_clf_pred = tuned_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "    conf_matrix = confusion_matrix(y_eval,tuned_clf_pred) #build confusion matrix\n",
    "    precision = precision_score(y_eval,tuned_clf_pred) #calculate precision\n",
    "    recall = recall_score(y_eval,tuned_clf_pred) #calculate recall\n",
    "    f1 = f1_score(y_eval,tuned_clf_pred) #calculate f1\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval,tuned_clf_pred)\n",
    "    auc_score = auc(fpr, tpr) #calculate auc\n",
    "    accuracy = accuracy_score(y_eval,tuned_clf_pred) #calculate accuracy\n",
    "    class_eval['pred'] = tuned_clf_pred\n",
    "    class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "    print(conf_matrix)\n",
    "    print('precision = ' + str(precision))\n",
    "    print('recall = ' + str(recall))\n",
    "    print('f1 = ' + str(f1))\n",
    "    print('auc = ' + str(auc_score))\n",
    "    print('accuracy = ' + str(accuracy))\n",
    "    \n",
    "    return(best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval) #return metrics and pred vs actuals for each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiOXy6c6Dfqy"
   },
   "source": [
    "## Perform grid search and evaluation for each classifier and feature combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 564120,
     "status": "ok",
     "timestamp": 1571139991744,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "wuVvYlMCRjbJ",
    "outputId": "e66383e9-2c8c-4b4e-b54e-04ef04eccc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "{'C': 1, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.8120326541570113\n",
      "\n",
      "Evaluation data scores\n",
      "[[809  22]\n",
      " [ 44 146]]\n",
      "precision = 0.8690476190476191\n",
      "recall = 0.7684210526315789\n",
      "f1 = 0.8156424581005587\n",
      "auc = 0.8709734625372094\n",
      "accuracy = 0.9353574926542605\n",
      "df shape =  (1021, 4)\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.5963933476516136\n",
      "\n",
      "Evaluation data scores\n",
      "[[768  63]\n",
      " [ 73 117]]\n",
      "precision = 0.65\n",
      "recall = 0.6157894736842106\n",
      "f1 = 0.6324324324324324\n",
      "auc = 0.7699885996579897\n",
      "accuracy = 0.8667972575905974\n",
      "df shape =  (2042, 4)\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.46997170491154694\n",
      "\n",
      "Evaluation data scores\n",
      "[[825   6]\n",
      " [104  86]]\n",
      "precision = 0.9347826086956522\n",
      "recall = 0.45263157894736844\n",
      "f1 = 0.6099290780141844\n",
      "auc = 0.7227056811704352\n",
      "accuracy = 0.8922624877571009\n",
      "df shape =  (3063, 4)\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.1, 'kernel': 'linear', 'random_state': 42}\n",
      "Best grid search score =  0.8120439580625106\n",
      "\n",
      "Evaluation data scores\n",
      "[[812  19]\n",
      " [ 43 147]]\n",
      "precision = 0.8855421686746988\n",
      "recall = 0.7736842105263158\n",
      "f1 = 0.8258426966292135\n",
      "auc = 0.8754100956362024\n",
      "accuracy = 0.9392752203721841\n",
      "df shape =  (4084, 4)\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'penalty': 'l2', 'random_state': 42}\n",
      "Best grid search score =  0.8582166440588557\n",
      "\n",
      "Evaluation data scores\n",
      "[[818  13]\n",
      " [ 36 154]]\n",
      "precision = 0.9221556886227545\n",
      "recall = 0.8105263157894737\n",
      "f1 = 0.8627450980392157\n",
      "auc = 0.8974412565710304\n",
      "accuracy = 0.9520078354554359\n",
      "df shape =  (5105, 4)\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.6625650563311292\n",
      "\n",
      "Evaluation data scores\n",
      "[[779  52]\n",
      " [ 74 116]]\n",
      "precision = 0.6904761904761905\n",
      "recall = 0.6105263157894737\n",
      "f1 = 0.6480446927374303\n",
      "auc = 0.7739755525999114\n",
      "accuracy = 0.8765915768854065\n",
      "df shape =  (6126, 4)\n",
      "\n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'random_state': 42}\n",
      "Best grid search score =  0.4760717515401572\n",
      "\n",
      "Evaluation data scores\n",
      "[[823   8]\n",
      " [108  82]]\n",
      "precision = 0.9111111111111111\n",
      "recall = 0.43157894736842106\n",
      "f1 = 0.5857142857142857\n",
      "auc = 0.7109759959465451\n",
      "accuracy = 0.8863858961802155\n",
      "df shape =  (7147, 4)\n",
      "\n",
      "Best parameters\n",
      "{'C': 0.05, 'kernel': 'linear', 'random_state': 42}\n",
      "Best grid search score =  0.8474200601932967\n",
      "\n",
      "Evaluation data scores\n",
      "[[818  13]\n",
      " [ 34 156]]\n",
      "precision = 0.9230769230769231\n",
      "recall = 0.8210526315789474\n",
      "f1 = 0.8690807799442897\n",
      "auc = 0.9027044144657672\n",
      "accuracy = 0.9539666993143977\n",
      "df shape =  (8168, 4)\n",
      "time taken =  0:09:23.608582\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "current = datetime.now()\n",
    "\n",
    "tf = []     #initialise empty vectors to hold results\n",
    "name = []\n",
    "bp = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "tp = []\n",
    "p = []\n",
    "r = []\n",
    "f_1 = []\n",
    "auc_sc = []\n",
    "acc = []\n",
    "\n",
    "features = 'glove_300'\n",
    "\n",
    "trains = [train_vec_sum, train_vec_count_sum]\n",
    "evals = [eval_vec_sum, eval_vec_count_sum]\n",
    "descs = ['sum','sum_plusCounts']\n",
    "\n",
    "i=1\n",
    "for train, eval, desc in zip(trains, evals, descs):\n",
    "  \n",
    "  train = scale(train)\n",
    "  eval = scale(eval)\n",
    "\n",
    "  classifiers = [log_clf, dt_clf, rf_clf, svc_clf] #the classifiers that are to be tested\n",
    "  models = ['lr','dt','rf', 'svm'] #labels for identifying the results\n",
    "\n",
    "  x_train = train\n",
    "  y_train = train_labels\n",
    "\n",
    "\n",
    "\n",
    "  x_eval = eval\n",
    "  y_eval = eval_labels\n",
    "\n",
    "\n",
    "  for classifier, model in zip(classifiers,models):\n",
    "    \n",
    "    class_train = pd.DataFrame({'tweet_id':train_id,\n",
    "                                'class_column':y_train})\n",
    "    \n",
    "    class_eval = pd.DataFrame({'tweet_id':eval_id,\n",
    "                                'class_column':y_eval})    \n",
    "    \n",
    "    best_parameters, conf_matrix, precision, recall, f1, auc_score, accuracy, class_eval = search_grid(classifier, \n",
    "                                                                                                       model, \n",
    "                                                                                                       x_train, \n",
    "                                                                                                       y_train, \n",
    "                                                                                                       class_train, \n",
    "                                                                                                       x_eval, \n",
    "                                                                                                       y_eval, \n",
    "                                                                                                       class_eval)\n",
    "    #append the latest results to the vectors\n",
    "    tf_text = features + \"_\" + desc\n",
    "    tf = np.append(tf,tf_text)\n",
    "    name = np.append(name,model)\n",
    "    b = ';'.join('{} {}'.format(key, val) for key, val in best_parameters.items())\n",
    "    bp = np.append(bp,b)\n",
    "    tn = np.append(tn,conf_matrix[0][0])\n",
    "    fp = np.append(fp,conf_matrix[0][1])\n",
    "    fn = np.append(fn,conf_matrix[1][0])\n",
    "    tp = np.append(tp,conf_matrix[1][1])\n",
    "    p = np.append(p,precision)\n",
    "    r = np.append(r,recall)\n",
    "    f_1 = np.append(f_1,f1)\n",
    "    auc_sc = np.append(auc_sc,auc_score)\n",
    "    acc = np.append(acc,accuracy)\n",
    "\n",
    "    #col = train_file+'_'+model #build a column name\n",
    "    #class_eval.columns = ['tweet_id',col] #rename the columns\n",
    "    class_eval['model'] = model\n",
    "    class_eval['file'] = tf_text\n",
    "    if i==1: #if we are on the first iteration of the loop\n",
    "        df = class_eval.copy()\n",
    "    else: #if we are not on the first iteration f the loop\n",
    "        #df = pd.merge(df, class_eval, on='tweet_id')\n",
    "        df = df.append(class_eval)\n",
    "\n",
    "    i = i+1 #increment i\n",
    "    print('df shape = ', df.shape)\n",
    "\n",
    "print('time taken = ',datetime.now() - current) #print the time taken\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuAV-iLGDon6"
   },
   "source": [
    "## Create dataframe for performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1571140025301,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "g3Apx01rhdiU",
    "outputId": "fa8a0374-90f5-42ed-9e89-07b6dca3f528"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>name</th>\n",
       "      <th>bp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f_1</th>\n",
       "      <th>auc_sc</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 0.05;kernel linear;random_state 42</td>\n",
       "      <td>818.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.902704</td>\n",
       "      <td>0.953967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>818.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.922156</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.897441</td>\n",
       "      <td>0.952008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 0.1;kernel linear;random_state 42</td>\n",
       "      <td>812.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.875410</td>\n",
       "      <td>0.939275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 1;penalty l2;random_state 42</td>\n",
       "      <td>809.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.870973</td>\n",
       "      <td>0.935357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>779.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.773976</td>\n",
       "      <td>0.876592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>768.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>0.632432</td>\n",
       "      <td>0.769989</td>\n",
       "      <td>0.866797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>825.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.609929</td>\n",
       "      <td>0.722706</td>\n",
       "      <td>0.892262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>823.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.710976</td>\n",
       "      <td>0.886386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         tf name  ...    auc_sc       acc\n",
       "7  glove_300_sum_plusCounts  svm  ...  0.902704  0.953967\n",
       "4  glove_300_sum_plusCounts   lr  ...  0.897441  0.952008\n",
       "3             glove_300_sum  svm  ...  0.875410  0.939275\n",
       "0             glove_300_sum   lr  ...  0.870973  0.935357\n",
       "5  glove_300_sum_plusCounts   dt  ...  0.773976  0.876592\n",
       "1             glove_300_sum   dt  ...  0.769989  0.866797\n",
       "2             glove_300_sum   rf  ...  0.722706  0.892262\n",
       "6  glove_300_sum_plusCounts   rf  ...  0.710976  0.886386\n",
       "\n",
       "[8 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = pd.DataFrame({'tf':tf, #create a dataframe to hold the metrics\n",
    "                                'name':name,\n",
    "                               'bp':bp,\n",
    "                               'tn':tn,\n",
    "                               'fp':fp,\n",
    "                               'fn':fn,\n",
    "                               'tp':tp,\n",
    "                               'p':p,\n",
    "                               'r':r,\n",
    "                               'f_1':f_1,\n",
    "                               'auc_sc':auc_sc,\n",
    "                               'acc':acc})\n",
    "\n",
    "classifications.sort_values(by='f_1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fjk95XMOOJF"
   },
   "source": [
    "## Ensemble Classifier (vector sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9173,
     "status": "ok",
     "timestamp": 1571140104873,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "25dRKGpXOaoY",
    "outputId": "9e9556d7-636c-431d-cf7f-1cfb621bdd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[814  17]\n",
      " [ 45 145]]\n",
      "precision = 0.8950617283950617\n",
      "recall = 0.7631578947368421\n",
      "f1 = 0.8238636363636365\n",
      "auc = 0.8713503071758819\n",
      "accuracy = 0.9392752203721841\n",
      "df shape =  (9189, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(penalty='l2',C=1, random_state=42) #logistic regression with best hyperparameters\n",
    "svc_clf = SVC(C=0.1, kernel='linear', probability = True, random_state=42) #svm with best hyperparameters\n",
    "\n",
    "#create the ensemble\n",
    "e_clf = VotingClassifier(estimators=[('lr', log_clf), ('svm', svc_clf)],\n",
    "                         voting='soft', weights=[1, 1])\n",
    "\n",
    "\n",
    "features = 'glove_300'\n",
    "\n",
    " \n",
    "train = scale(train_vec_sum)\n",
    "eval = scale(eval_vec_sum)\n",
    "desc = 'sum'\n",
    "model = 'ensemble'\n",
    "\n",
    "x_train = train\n",
    "y_train = train_labels\n",
    "\n",
    "x_eval = eval\n",
    "y_eval = eval_labels\n",
    "\n",
    "\n",
    "    \n",
    "class_train = pd.DataFrame({'tweet_id':train_id,\n",
    "                            'class_column':y_train})\n",
    "\n",
    "class_eval = pd.DataFrame({'tweet_id':eval_id,\n",
    "                            'class_column':y_eval}) \n",
    "\n",
    "\n",
    "e_clf = e_clf.fit(x_train, y_train) #fit the ensemble\n",
    "\n",
    "e_clf_pred = e_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "conf_matrix = confusion_matrix(y_eval,e_clf_pred) #build confusion matrix\n",
    "precision = precision_score(y_eval,e_clf_pred) #calculate precision\n",
    "recall = recall_score(y_eval,e_clf_pred) #calculate recall\n",
    "f1 = f1_score(y_eval,e_clf_pred) #calculate f1\n",
    "fpr, tpr, thresholds = roc_curve(y_eval,e_clf_pred)\n",
    "auc_score = auc(fpr, tpr) #calculate auc\n",
    "accuracy = accuracy_score(y_eval,e_clf_pred) #calculate accuracy\n",
    "class_eval['pred'] = e_clf_pred\n",
    "class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "print(conf_matrix)\n",
    "print('precision = ' + str(precision))\n",
    "print('recall = ' + str(recall))\n",
    "print('f1 = ' + str(f1))\n",
    "print('auc = ' + str(auc_score))\n",
    "print('accuracy = ' + str(accuracy))\n",
    "\n",
    "#append the latest results to the vectors\n",
    "tf_text = features + \"_\" + desc\n",
    "tf = np.append(tf,tf_text)\n",
    "name = np.append(name,'ensemble (lr,svc)')\n",
    "bp = np.append(bp,'ensemble')\n",
    "tn = np.append(tn,conf_matrix[0][0])\n",
    "fp = np.append(fp,conf_matrix[0][1])\n",
    "fn = np.append(fn,conf_matrix[1][0])\n",
    "tp = np.append(tp,conf_matrix[1][1])\n",
    "p = np.append(p,precision)\n",
    "r = np.append(r,recall)\n",
    "f_1 = np.append(f_1,f1)\n",
    "auc_sc = np.append(auc_sc,auc_score)\n",
    "acc = np.append(acc,accuracy)\n",
    "\n",
    "class_eval['model'] = 'ensemble (lr,svc)'\n",
    "class_eval['file'] = tf_text\n",
    "\n",
    "df = df.append(class_eval) #merge the latest predictions for each tweet using this classifier\n",
    "print('df shape = ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rm0qMnT3Oc_Y"
   },
   "source": [
    "## Ensemble Classifier 2 (vector sums plus counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7701,
     "status": "ok",
     "timestamp": 1571140121670,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "cAhCCKa-OgS4",
    "outputId": "c32fdd30-d942-437d-c590-a23b463db5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[819  12]\n",
      " [ 38 152]]\n",
      "precision = 0.926829268292683\n",
      "recall = 0.8\n",
      "f1 = 0.8587570621468926\n",
      "auc = 0.8927797833935018\n",
      "accuracy = 0.951028403525955\n",
      "df shape =  (10210, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(penalty='l2',C=0.05, random_state=42) #logistic regression with best hyperparameters\n",
    "svc_clf = SVC(C=0.05, kernel='linear', probability = True, random_state=42) #svm with best hyperparameters\n",
    "\n",
    "#create the ensemble\n",
    "e_clf = VotingClassifier(estimators=[('lr', log_clf), ('svm', svc_clf)],\n",
    "                         voting='soft', weights=[1, 1])\n",
    "\n",
    "\n",
    "features = 'glove_300'\n",
    "\n",
    " \n",
    "train = scale(train_vec_count_sum)\n",
    "eval = scale(eval_vec_count_sum)\n",
    "desc = 'sum_plusCounts'\n",
    "model = 'ensemble'\n",
    "\n",
    "x_train = train\n",
    "y_train = train_labels\n",
    "\n",
    "x_eval = eval\n",
    "y_eval = eval_labels\n",
    "\n",
    "\n",
    "    \n",
    "class_train = pd.DataFrame({'tweet_id':train_id,\n",
    "                            'class_column':y_train})\n",
    "\n",
    "class_eval = pd.DataFrame({'tweet_id':eval_id,\n",
    "                            'class_column':y_eval}) \n",
    "\n",
    "\n",
    "e_clf = e_clf.fit(x_train, y_train) #fit the ensemble\n",
    "\n",
    "e_clf_pred = e_clf.predict(x_eval) #predict using evaluation data with best parameters\n",
    "conf_matrix = confusion_matrix(y_eval,e_clf_pred) #build confusion matrix\n",
    "precision = precision_score(y_eval,e_clf_pred) #calculate precision\n",
    "recall = recall_score(y_eval,e_clf_pred) #calculate recall\n",
    "f1 = f1_score(y_eval,e_clf_pred) #calculate f1\n",
    "fpr, tpr, thresholds = roc_curve(y_eval,e_clf_pred)\n",
    "auc_score = auc(fpr, tpr) #calculate auc\n",
    "accuracy = accuracy_score(y_eval,e_clf_pred) #calculate accuracy\n",
    "class_eval['pred'] = e_clf_pred\n",
    "class_eval = class_eval.drop('class_column', axis=1) #join predictions onto actuals\n",
    "print(conf_matrix)\n",
    "print('precision = ' + str(precision))\n",
    "print('recall = ' + str(recall))\n",
    "print('f1 = ' + str(f1))\n",
    "print('auc = ' + str(auc_score))\n",
    "print('accuracy = ' + str(accuracy))\n",
    "\n",
    "#append the latest results to the vectors\n",
    "tf_text = features + \"_\" + desc\n",
    "tf = np.append(tf,tf_text)\n",
    "name = np.append(name,'ensemble (lr,svc)')\n",
    "bp = np.append(bp,'ensemble')\n",
    "tn = np.append(tn,conf_matrix[0][0])\n",
    "fp = np.append(fp,conf_matrix[0][1])\n",
    "fn = np.append(fn,conf_matrix[1][0])\n",
    "tp = np.append(tp,conf_matrix[1][1])\n",
    "p = np.append(p,precision)\n",
    "r = np.append(r,recall)\n",
    "f_1 = np.append(f_1,f1)\n",
    "auc_sc = np.append(auc_sc,auc_score)\n",
    "acc = np.append(acc,accuracy)\n",
    "\n",
    "class_eval['model'] = 'ensemble (lr,svc)'\n",
    "class_eval['file'] = tf_text\n",
    "\n",
    "df = df.append(class_eval) #merge the latest predictions for each tweet using this classifier\n",
    "print('df shape = ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1571140135238,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "uhukNhNTOk-g",
    "outputId": "5e92d512-f6bf-4e94-9c9b-38f72dfd7014"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>name</th>\n",
       "      <th>bp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f_1</th>\n",
       "      <th>auc_sc</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 0.05;kernel linear;random_state 42</td>\n",
       "      <td>818.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.902704</td>\n",
       "      <td>0.953967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 0.05;penalty l2;random_state 42</td>\n",
       "      <td>818.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.922156</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.897441</td>\n",
       "      <td>0.952008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>ensemble (lr,svc)</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>819.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.892780</td>\n",
       "      <td>0.951028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>svm</td>\n",
       "      <td>C 0.1;kernel linear;random_state 42</td>\n",
       "      <td>812.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.875410</td>\n",
       "      <td>0.939275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>ensemble (lr,svc)</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>814.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.895062</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.871350</td>\n",
       "      <td>0.939275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>lr</td>\n",
       "      <td>C 1;penalty l2;random_state 42</td>\n",
       "      <td>809.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.870973</td>\n",
       "      <td>0.935357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>779.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.773976</td>\n",
       "      <td>0.876592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>dt</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>768.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>0.632432</td>\n",
       "      <td>0.769989</td>\n",
       "      <td>0.866797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glove_300_sum</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>825.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.609929</td>\n",
       "      <td>0.722706</td>\n",
       "      <td>0.892262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>glove_300_sum_plusCounts</td>\n",
       "      <td>rf</td>\n",
       "      <td>criterion entropy;random_state 42</td>\n",
       "      <td>823.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.710976</td>\n",
       "      <td>0.886386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         tf               name  ...    auc_sc       acc\n",
       "7  glove_300_sum_plusCounts                svm  ...  0.902704  0.953967\n",
       "4  glove_300_sum_plusCounts                 lr  ...  0.897441  0.952008\n",
       "9  glove_300_sum_plusCounts  ensemble (lr,svc)  ...  0.892780  0.951028\n",
       "3             glove_300_sum                svm  ...  0.875410  0.939275\n",
       "8             glove_300_sum  ensemble (lr,svc)  ...  0.871350  0.939275\n",
       "0             glove_300_sum                 lr  ...  0.870973  0.935357\n",
       "5  glove_300_sum_plusCounts                 dt  ...  0.773976  0.876592\n",
       "1             glove_300_sum                 dt  ...  0.769989  0.866797\n",
       "2             glove_300_sum                 rf  ...  0.722706  0.892262\n",
       "6  glove_300_sum_plusCounts                 rf  ...  0.710976  0.886386\n",
       "\n",
       "[10 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = pd.DataFrame({'tf':tf, #create a dataframe to hold the metrics\n",
    "                                'name':name,\n",
    "                               'bp':bp,\n",
    "                               'tn':tn,\n",
    "                               'fp':fp,\n",
    "                               'fn':fn,\n",
    "                               'tp':tp,\n",
    "                               'p':p,\n",
    "                               'r':r,\n",
    "                               'f_1':f_1,\n",
    "                               'auc_sc':auc_sc,\n",
    "                               'acc':acc})\n",
    "\n",
    "classifications.sort_values(by='f_1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4onrRr51DxVs"
   },
   "source": [
    "## Save the performance metrics and the predictions per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6847,
     "status": "ok",
     "timestamp": 1571140152006,
     "user": {
      "displayName": "Scott Kilgariff",
      "photoUrl": "",
      "userId": "04893934711112937862"
     },
     "user_tz": -60
    },
    "id": "Nd5geDP5-rbM",
    "outputId": "1f9b6741-a5e0-4154-b88c-13a242d9bf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape =  (10210, 4)\n",
      "Copying file:///tmp/classifications.pickle [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.1 KiB/  2.1 KiB]                                                \n",
      "Operation completed over 1 objects/2.1 KiB.                                      \n",
      "Copying file:///tmp/df.pickle [Content-Type=application/octet-stream]...\n",
      "/ [1 files][478.0 KiB/478.0 KiB]                                                \n",
      "Operation completed over 1 objects/478.0 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "classifications.to_pickle('/tmp/classifications.pickle')\n",
    "df.to_pickle('/tmp/df.pickle')\n",
    "print('df shape = ', df.shape)\n",
    "\n",
    "!gsutil cp /tmp/classifications.pickle gs://dissertation_bucket/classify_embeddings_glove/\n",
    "!gsutil cp /tmp/df.pickle gs://dissertation_bucket/classify_embeddings_glove/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13_COLAB_classify_embeddings_glove.ipynb",
   "provenance": [
    {
     "file_id": "1BQ5M4KxX_aUyckDRcsXYV3oN6nbqxfRd",
     "timestamp": 1569271535373
    },
    {
     "file_id": "1_47lBeiEsM6DoYJRGBIOuMIJBGc0wDpc",
     "timestamp": 1569177794989
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
