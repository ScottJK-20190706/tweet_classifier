{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_matrix(folders):\n",
    "    i=1 #counter\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        location = 'features/' + folder + '*.pickle'\n",
    "        for file in glob.glob(location):                         #loop through each saved file\n",
    "            #print(file)\n",
    "            df = pd.read_pickle(file)                                        #read each file into a dataframe\n",
    "            df['tweet_id'] = df.tweet_id.astype(str)\n",
    "            if (i==1):                                                       #if we're on the first iteration\n",
    "                ngrams = df\n",
    "            else:\n",
    "                ngrams = ngrams.append(df)                             #append if we're not on the first iiteration\n",
    "            i = i+1\n",
    "\n",
    "    ngrams['count'] = 1                                               #add a filed to count occurences\n",
    "    ngrams.columns = ['feature','tweet_ids','value']                  #change the column names\n",
    "\n",
    "    print('aggregating')\n",
    "    features_agg = ngrams.groupby(['tweet_ids', 'feature'])['value'].count().reset_index() #aggregate the full dataframe\n",
    "    x = features_agg\n",
    "    x = x.groupby(['feature'])['value'].sum().reset_index()\n",
    "    print(x.shape)\n",
    "    print(x.columns)\n",
    "    x = x[x['value']>839//20]                                    #term must exist in at least 5% of the smaller class\n",
    "    frequent_features = np.unique(x.feature)\n",
    "    features_agg = features_agg[features_agg.feature.isin(frequent_features)]\n",
    "\n",
    "    print('pivoting')\n",
    "    features_agg_pivot = features_agg.pivot_table(index=['tweet_ids'], columns='feature', values='value').reset_index() #pivot\n",
    "    print('filling nans')\n",
    "    features_agg_pivot.fillna(0, inplace=True)                                #replace nans with zeros\n",
    "    print(features_agg_pivot.shape) #for convenience\n",
    "    print(len(np.unique(features_agg_pivot.tweet_ids)))\n",
    "    return(features_agg_pivot,features_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ngrams matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_word_4grams/\n",
      "tweet_word_3grams/\n",
      "tweet_word_2grams/\n",
      "tweet_word_1grams/\n",
      "aggregating\n",
      "(95212, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(4991, 221)\n",
      "4991\n",
      "tweet_char_4grams/\n",
      "tweet_char_3grams/\n",
      "tweet_char_2grams/\n",
      "tweet_char_1grams/\n",
      "aggregating\n",
      "(30736, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(5097, 2886)\n",
      "5097\n",
      "tweet_word_4grams/\n",
      "tweet_word_3grams/\n",
      "tweet_word_2grams/\n",
      "tweet_word_1grams/\n",
      "bio_word_4grams/\n",
      "bio_word_3grams/\n",
      "bio_word_2grams/\n",
      "bio_word_1grams/\n",
      "aggregating\n",
      "(176015, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(5077, 414)\n",
      "5077\n",
      "tweet_char_4grams/\n",
      "tweet_char_3grams/\n",
      "tweet_char_2grams/\n",
      "tweet_char_1grams/\n",
      "bio_char_4grams/\n",
      "bio_char_3grams/\n",
      "bio_char_2grams/\n",
      "bio_char_1grams/\n",
      "aggregating\n",
      "(56674, 2)\n",
      "Index(['feature', 'value'], dtype='object')\n",
      "pivoting\n",
      "filling nans\n",
      "(5104, 5205)\n",
      "5104\n"
     ]
    }
   ],
   "source": [
    "tweet_word_folders = ['tweet_word_4grams/',\n",
    "           'tweet_word_3grams/',\n",
    "           'tweet_word_2grams/',\n",
    "           'tweet_word_1grams/']\n",
    "\n",
    "tweet_char_folders = ['tweet_char_4grams/',\n",
    "           'tweet_char_3grams/',\n",
    "           'tweet_char_2grams/',\n",
    "           'tweet_char_1grams/']\n",
    "\n",
    "bio_word_folders = ['bio_word_4grams/',\n",
    "           'bio_word_3grams/',\n",
    "           'bio_word_2grams/',\n",
    "           'bio_word_1grams/']\n",
    "\n",
    "bio_char_folders = ['bio_char_4grams/',\n",
    "           'bio_char_3grams/',\n",
    "           'bio_char_2grams/',\n",
    "           'bio_char_1grams/']\n",
    "\n",
    "tweetbio_word_folders = np.append(tweet_word_folders,bio_word_folders)\n",
    "tweetbio_char_folders = np.append(tweet_char_folders,bio_char_folders)\n",
    "\n",
    "tweet_w_ngrams_matrix, tweet_w_ngrams_list = ngram_matrix(tweet_word_folders) #word ngrams in tweets\n",
    "tweet_c_ngrams_matrix, tweet_c_ngrams_list = ngram_matrix(tweet_char_folders) #char ngrams in tweets\n",
    "tweetbio_w_ngrams_matrix, tweetbio_w_ngrams_list = ngram_matrix(tweetbio_word_folders) #word ngrams in bios\n",
    "tweetbio_c_ngrams_matrix, tweetbio_c_ngrams_list = ngram_matrix(tweetbio_char_folders) #char ngrams in bios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data to obtain classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "train_data['Tweet ID'] = train_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "eval_data = pd.read_pickle('pickle_files\\eval_data_formatted.pickle')\n",
    "eval_data['Tweet ID'] = eval_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "tweet_class = all_data.loc[:,['Tweet ID','class']] #get the id and the class column\n",
    "tweet_class.columns = ['Tweet ID','class_column'] #change name of 'class' to 'class_column' ('class' might be a unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for performing Chi^2 test on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2test(df):    \n",
    "    df_class = pd.merge(df,tweet_class, left_on='tweet_ids',right_on='Tweet ID', how='inner') #get the class for instances in df\n",
    "    Y = df_class.loc[:,'class_column'].copy()                                                 #create list of classes\n",
    "    df_class = df_class.drop(['class_column','Tweet ID','tweet_ids'], axis=1)                 #drop class and id features from df\n",
    "    X = df_class.values                                                                       #create matrix of features\n",
    "\n",
    "    from sklearn.feature_selection import chi2\n",
    "    current = datetime.now()\n",
    "    x = chi2(X,Y)                                             #perfrom chi2 test\n",
    "    time_taken = datetime.now() - current\n",
    "    print('time to process chi 2 = ',time_taken)\n",
    "    \n",
    "    important_cols = df_class.columns[x[1]<0.05]              #create list of features where p-value < 0.05\n",
    "    \n",
    "    return(important_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for reducing features based on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df,coeff):\n",
    "    current = datetime.now()\n",
    "    print('Creating correlation matrix')\n",
    "    corr = df.iloc[:,1:].corr()             #create the correlayion matrix\n",
    "    x = datetime.now() - current\n",
    "    print('time to process correlation matrix = ',x)\n",
    "\n",
    "    x = datetime.now() - current\n",
    "    ids = df.iloc[:,:1]                     #create a list for the ids to be added back later\n",
    "    print('number of features = ',len(corr.index))#print the number of features to be analysed\n",
    "    i = 1\n",
    "    for m in corr.index:                          #loop through each row\n",
    "        for n in corr.index:                #loop through each column\n",
    "            if m!=n:                        #ignore if row equals column\n",
    "                try:\n",
    "                    r = corr.loc[m,n]           #get correlation value at intersection\n",
    "                    if r>coeff:                   #if correlation value is greater than 0.9\n",
    "                        corr = corr.drop(n, axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "        if (i//100==i/100):\n",
    "            print('progress = ',str(i))\n",
    "        i = i+1\n",
    "\n",
    "    df = df[corr.columns]\n",
    "    df['tweet_id'] = ids\n",
    "    x = datetime.now() - current\n",
    "    print('time to check all features = ',x)\n",
    "\n",
    "    return(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to combine 'chi2test' and 'correlation' functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2corr(df_matrix):\n",
    "    df_features = chi2test(df_matrix)                                               #get list of features where p<0.05\n",
    "    df_features = np.append('tweet_ids',df_features)                                #append tweet_id to feature list\n",
    "    df_chi = df_matrix[df_features]                                                 #reduce features using chi2test output\n",
    "    df_chi_corr = correlation(df_chi,0.9)                                           #reduce features based on correlation\n",
    "    return(df_chi_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform chi^2 test and corr test and reduce features based on result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process chi 2 =  0:00:00.171837\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:00:00.349690\n",
      "number of features =  168\n",
      "progress =  100\n",
      "time to check all features =  0:00:01.626593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "tweet_w_features = chi2corr(tweet_w_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process chi 2 =  0:00:00.062489\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:00:37.867058\n",
      "number of features =  1817\n",
      "progress =  100\n"
     ]
    }
   ],
   "source": [
    "tweet_c_features = chi2corr(tweet_c_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetbio_w_features = chi2corr(tweetbio_w_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process chi 2 =  0:00:00.150595\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:01:10.571408\n",
      "number of features =  2532\n",
      "progress =  100\n",
      "progress =  200\n",
      "progress =  300\n",
      "progress =  400\n",
      "progress =  500\n",
      "progress =  600\n",
      "progress =  700\n",
      "progress =  800\n",
      "progress =  900\n",
      "progress =  1000\n",
      "progress =  1100\n",
      "progress =  1200\n",
      "progress =  1300\n",
      "progress =  1400\n",
      "progress =  1500\n",
      "progress =  1600\n",
      "progress =  1700\n",
      "progress =  1800\n",
      "progress =  1900\n",
      "progress =  2000\n",
      "progress =  2100\n",
      "progress =  2200\n",
      "progress =  2300\n",
      "progress =  2400\n",
      "progress =  2500\n",
      "time to check all features =  0:05:50.718320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "tweetbio_c_features = chi2corr(tweetbio_c_ngrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4991, 124)\n",
      "(5097, 1156)\n",
      "(5077, 193)\n",
      "(5104, 1804)\n"
     ]
    }
   ],
   "source": [
    "#shapes\n",
    "print(tweet_w_features.shape)\n",
    "print(tweet_c_features.shape)\n",
    "print(tweetbio_w_features.shape)\n",
    "print(tweetbio_c_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating feature term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termfrequency(df):\n",
    "    df_tf = df.copy()\n",
    "    df_tf['sum_of_counts'] = df_tf.sum(axis=1) #get sum of counts across axis 0\n",
    "    for c in df_tf.columns: #loop through each column\n",
    "        if c != 'tweet_id': #skip the tweet id column\n",
    "            df_tf[c] = df_tf[c]/df_tf['sum_of_counts'] #divide column value by the sum of row value\n",
    "    df_tf = df_tf.drop('sum_of_counts', axis=1) #drop the sum\n",
    "    df_tf = df_tf.fillna(0)\n",
    "    \n",
    "    return(df_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the feature term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w_features_tf = termfrequency(tweet_w_features)\n",
    "tweet_c_features_tf = termfrequency(tweet_c_features)\n",
    "tweetbio_w_features_tf = termfrequency(tweetbio_w_features)\n",
    "tweetbio_c_features_tf = termfrequency(tweetbio_c_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4991, 124)\n",
      "(5097, 1156)\n",
      "(5077, 193)\n",
      "(5104, 1804)\n"
     ]
    }
   ],
   "source": [
    "#shapes\n",
    "print(tweet_w_features_tf.shape)\n",
    "print(tweet_c_features_tf.shape)\n",
    "print(tweetbio_w_features_tf.shape)\n",
    "print(tweetbio_c_features_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for converting tf to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(df_tf):    \n",
    "    df_tfidf = df_tf.copy()\n",
    "    for c in df_tfidf.columns: #loop through each column\n",
    "        if c != 'tweet_id': #skip the tweet id column\n",
    "            df_tfidf[c] = df_tfidf[c]*(np.log(df_tfidf.shape[0]/(df_tfidf[c][df_tfidf[c]>0].count()))+1)\n",
    "    return(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w_features_tfidf = idf(tweet_w_features_tf)\n",
    "tweet_c_features_tfidf = idf(tweet_c_features_tf)\n",
    "tweetbio_w_features_tfidf = idf(tweetbio_w_features_tf)\n",
    "tweetbio_c_features_tfidf = idf(tweetbio_c_features_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4991, 124)\n",
      "(5097, 1156)\n",
      "(5077, 193)\n",
      "(5104, 1804)\n"
     ]
    }
   ],
   "source": [
    "#shapes\n",
    "print(tweet_w_features_tfidf.shape)\n",
    "print(tweet_c_features_tfidf.shape)\n",
    "print(tweetbio_w_features_tfidf.shape)\n",
    "print(tweetbio_c_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivoting\n",
      "filling nans\n",
      "(5104, 16)\n",
      "5104\n",
      "Index(['tweet_ids', '1char_token_count', 'ave_chars_token', 'caps_count',\n",
      "       'character_count', 'followers_count', 'following_count',\n",
      "       'hashtag_count', 'masked_count', 'mention_count', 'modals_count',\n",
      "       'posted_tweets_count', 'punctuation_count', 'quotes_count',\n",
      "       'token_count', 'url_count'],\n",
      "      dtype='object', name='feature')\n",
      "time to process chi 2 =  0:00:00.003987\n",
      "Creating correlation matrix\n",
      "time to process correlation matrix =  0:00:00.001995\n",
      "number of features =  11\n",
      "time to check all features =  0:00:00.049867\n",
      "Index(['ave_chars_token', 'caps_count', 'followers_count', 'following_count',\n",
      "       'mention_count', 'posted_tweets_count', 'punctuation_count',\n",
      "       'quotes_count', 'url_count', 'tweet_id'],\n",
      "      dtype='object', name='feature')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "count_features = pd.read_pickle('features/count_features/count_features.pickle') #get the count features\n",
    "count_features['tweet_ids'] = count_features.tweet_ids.astype(str)\n",
    "\n",
    "#features_agg = features_agg.append(count_features)                          #append count features on to ngrams\n",
    "\n",
    "print('pivoting')\n",
    "count_features_pivot = count_features.pivot_table(index=['tweet_ids'], columns='feature', values='value').reset_index() #pivot\n",
    "print('filling nans')\n",
    "count_features_pivot.fillna(0, inplace=True)                                #replace nans with zeros\n",
    "print(count_features_pivot.shape) #for convenience\n",
    "print(len(np.unique(count_features_pivot.tweet_ids)))\n",
    "\n",
    "print(count_features_pivot.columns)\n",
    "count_features = chi2corr(count_features_pivot) #select important features\n",
    "print(count_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5104, 1288)\n",
      "(5104, 1288)\n",
      "(5104, 1288)\n",
      "(5104, 2005)\n",
      "(5104, 2005)\n",
      "(5104, 2005)\n"
     ]
    }
   ],
   "source": [
    "df_tweet = pd.merge(count_features, tweet_w_features, on='tweet_id', how='left').fillna(0)\n",
    "df_tweet = pd.merge(df_tweet, tweet_c_features, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "df_tweet_tf = pd.merge(count_features, tweet_w_features_tf, on='tweet_id', how='left').fillna(0)\n",
    "df_tweet_tf = pd.merge(df_tweet_tf, tweet_c_features_tf, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "df_tweet_tfidf = pd.merge(count_features, tweet_w_features_tfidf, on='tweet_id', how='left').fillna(0)\n",
    "df_tweet_tfidf = pd.merge(df_tweet_tfidf, tweet_c_features_tfidf, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "df_tweetbio = pd.merge(count_features, tweetbio_w_features, on='tweet_id', how='left').fillna(0)\n",
    "df_tweetbio = pd.merge(df_tweetbio, tweetbio_c_features, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "df_tweetbio_tf = pd.merge(count_features, tweetbio_w_features_tf, on='tweet_id', how='left').fillna(0)\n",
    "df_tweetbio_tf = pd.merge(df_tweetbio_tf, tweetbio_c_features_tf, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "df_tweetbio_tfidf = pd.merge(count_features, tweetbio_w_features_tfidf, on='tweet_id', how='left').fillna(0)\n",
    "df_tweetbio_tfidf = pd.merge(df_tweetbio_tfidf, tweetbio_c_features_tfidf, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "print(df_tweet.shape)\n",
    "print(df_tweet_tf.shape)\n",
    "print(df_tweet_tfidf.shape)\n",
    "print(df_tweetbio.shape)\n",
    "print(df_tweetbio_tf.shape)\n",
    "print(df_tweetbio_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to split features matrix into train and eval for tweet-only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    c_f = np.delete(count_features.columns, np.where(count_features.columns=='tweet_id')) #get the count features column names\n",
    "    features = df.copy()\n",
    "    tweet_features_class = pd.merge(features,tweet_class,left_on='tweet_id',right_on='Tweet ID')\n",
    "    tweet_features_class = tweet_features_class.drop('Tweet ID', axis=1)\n",
    "    tweet_features_train = tweet_features_class[tweet_features_class.tweet_id.isin(train_data['Tweet ID'])]\n",
    "    tweet_features_eval = tweet_features_class[tweet_features_class.tweet_id.isin(eval_data['Tweet ID'])]\n",
    "    tweet_features_train_nc = tweet_features_train.drop(c_f,axis=1)\n",
    "    tweet_features_eval_nc = tweet_features_eval.drop(c_f,axis=1)\n",
    "    return(tweet_features_train,tweet_features_eval,tweet_features_train_nc,tweet_features_eval_nc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_train, df_tweet_eval, df_tweet_train_nc, df_tweet_eval_nc= split(count_features)\n",
    "df_tweet_train.to_pickle('features/df_tweet_count_features_train.pickle')\n",
    "df_tweet_eval.to_pickle('features/df_tweet_count_features_eval.pickle')\n",
    "\n",
    "df_tweet_train, df_tweet_eval, df_tweet_train_nc, df_tweet_eval_nc = split(df_tweet)\n",
    "df_tweet_train.to_pickle('features/df_tweet_train.pickle')\n",
    "df_tweet_eval.to_pickle('features/df_tweet_eval.pickle')\n",
    "df_tweet_train_nc.to_pickle('features/df_tweet_train_nc.pickle')\n",
    "df_tweet_eval_nc.to_pickle('features/df_tweet_eval_nc.pickle')\n",
    "\n",
    "df_tweet_tf_train, df_tweet_tf_eval, df_tweet_tf_train_nc, df_tweet_tf_eval_nc = split(df_tweet_tf)\n",
    "df_tweet_tf_train.to_pickle('features/df_tweet_tf_train.pickle')\n",
    "df_tweet_tf_eval.to_pickle('features/df_tweet_tf_eval.pickle')\n",
    "df_tweet_tf_train_nc.to_pickle('features/df_tweet_tf_train_nc.pickle')\n",
    "df_tweet_tf_eval_nc.to_pickle('features/df_tweet_tf_eval_nc.pickle')\n",
    "\n",
    "df_tweet_tfidf_train, df_tweet_tfidf_eval, df_tweet_tfidf_train_nc, df_tweet_tfidf_eval_nc = split(df_tweet_tfidf)\n",
    "df_tweet_tfidf_train.to_pickle('features/df_tweet_tfidf_train.pickle')\n",
    "df_tweet_tfidf_eval.to_pickle('features/df_tweet_tfidf_eval.pickle')\n",
    "df_tweet_tfidf_train_nc.to_pickle('features/df_tweet_tfidf_train_nc.pickle')\n",
    "df_tweet_tfidf_eval_nc.to_pickle('features/df_tweet_tfidf_eval_nc.pickle')\n",
    "\n",
    "df_tweetbio_train, df_tweetbio_eval, df_tweetbio_train_nc, df_tweetbio_eval_nc = split(df_tweetbio)\n",
    "df_tweetbio_train.to_pickle('features/df_tweetbio_train.pickle')\n",
    "df_tweetbio_eval.to_pickle('features/df_tweetbio_eval.pickle')\n",
    "df_tweetbio_train_nc.to_pickle('features/df_tweetbio_train_nc.pickle')\n",
    "df_tweetbio_eval_nc.to_pickle('features/df_tweetbio_eval_nc.pickle')\n",
    "\n",
    "df_tweetbio_tf_train, df_tweetbio_tf_eval, df_tweetbio_tf_train_nc, df_tweetbio_tf_eval_nc = split(df_tweetbio_tf)\n",
    "df_tweetbio_tf_train.to_pickle('features/df_tweetbio_tf_train.pickle')\n",
    "df_tweetbio_tf_eval.to_pickle('features/df_tweetbio_tf_eval.pickle')\n",
    "df_tweetbio_tf_train_nc.to_pickle('features/df_tweetbio_tf_train_nc.pickle')\n",
    "df_tweetbio_tf_eval_nc.to_pickle('features/df_tweetbio_tf_eval_nc.pickle')\n",
    "\n",
    "df_tweetbio_tfidf_train, df_tweetbio_tfidf_eval, df_tweetbio_tfidf_train_nc, df_tweetbio_tfidf_eval_nc = split(df_tweetbio_tfidf)\n",
    "df_tweetbio_tfidf_train.to_pickle('features/df_tweetbio_tfidf_train.pickle')\n",
    "df_tweetbio_tfidf_eval.to_pickle('features/df_tweetbio_tfidf_eval.pickle')\n",
    "df_tweetbio_tfidf_train_nc.to_pickle('features/df_tweetbio_tfidf_train_nc.pickle')\n",
    "df_tweetbio_tfidf_eval_nc.to_pickle('features/df_tweetbio_tfidf_eval_nc.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
