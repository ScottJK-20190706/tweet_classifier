{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and evaluation into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('pickle_files/eval_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].fillna('')\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Tweet'] = [p.sub('', x) for x in all_data['Tweet'].tolist()] #remove the punctuation\n",
    "for i in all_data.index:\n",
    "    #print(i)\n",
    "    #all_data.loc[i,'Bio'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       matt is this the same nicola sturgeon thats be...\n",
       "1            you know nothing of nicola sturgeon atkinson\n",
       "2       sturgeon letter to new pm demands essential al...\n",
       "3       scotland had a ref and lost they need permissi...\n",
       "4               nicola sturgeon depressing numpty go away\n",
       "                              ...                        \n",
       "5099    you really fancy your bara   nearly as bad as ...\n",
       "5100    good have her arrested for treason or hate cri...\n",
       "5101    great thread\\nsturgeonlucas already work well ...\n",
       "5102    an english nationalist conservative party woul...\n",
       "5103    well see it accompanies sturgeons preparations...\n",
       "Name: Tweet, Length: 5104, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for i in all_data.index:\n",
    "    for w in range(0,len(all_data.Tweet[i])):\n",
    "        all_data.Tweet[i][w] = ps.stem(all_data.Tweet[i][w])\n",
    "\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bio                     Currently obsessed with AI in #Martech and #Br...\n",
       "Hashtags                                                                 \n",
       "Location                                               Bray-on-Thames, UK\n",
       "Location Coordinates                                                 None\n",
       "Mentions                                                           syrpis\n",
       "Name                                                               opento\n",
       "Number Following                                                      937\n",
       "Number of Followers                                                  1899\n",
       "Number of Tweets                                                    35911\n",
       "Retweeted ID                                                          NaN\n",
       "Time                                       Fri Jul 26 12:45:01 +0000 2019\n",
       "Tweet                   [great, thread, sturgeonluca, alreadi, work, w...\n",
       "Tweet ID                                              1154734367517945856\n",
       "Username                                                 Sandra Pickering\n",
       "Web                                                                      \n",
       "class                                                                   0\n",
       "Name: 5101, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.iloc[5101,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(prog, folder, n_gram_size,file):\n",
    "\n",
    "    tweet_ids = [] #initialise list of tweet ids\n",
    "    n_grams = [] #initialise list of n_grams\n",
    "\n",
    "\n",
    "\n",
    "    progress_dumps = prog #memory becomes a problem with this code.  Set a value n whereby the processed lists of tweets will be saved\n",
    "                         #to disk every n tweets and memory will be cleared***\n",
    "    location = folder + '*.pickle'\n",
    "    print(folder)\n",
    "    x = glob.glob(folder)\n",
    "    print(len(x))\n",
    "    if len(x)<2:\n",
    "        for i in all_data.index:\n",
    "\n",
    "            tweet_id = all_data['Tweet ID'][i]                                  #get the tweet id\n",
    "            tokens = all_data.Tweet[i]                                          #get the associated tokens\n",
    "            n=n_gram_size                                                   #set the n_gram length***\n",
    "            if len(all_data.Tweet[i])==0:\n",
    "                n_grams = np.append(n_grams,'zzzz')                                 #if there are no tokens add \"zzzz\" for this tweet\n",
    "                tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            else:\n",
    "                for c in range(0,len(all_data.Tweet[i])-n+1):\n",
    "                    n_gram = ' '.join(all_data.Tweet[i][c:c+n])\n",
    "                    n_grams = np.append(n_grams,n_gram)               #add n_gram to list\n",
    "                    tweet_ids = np.append(tweet_ids,tweet_id) \n",
    "\n",
    "            if (i//progress_dumps==i/progress_dumps):                       #save progress and clear memory\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'          #set folder and filename***\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})                       #create dataframe\n",
    "                df.to_pickle(filename)                                      #save\n",
    "                del df                                                      #clear memory\n",
    "                tweet_ids = []                                              #reinitialise list of tweet ids\n",
    "                n_grams = []                                                #reinitialise list of n_grams\n",
    "                print(i)                                                    #print progress\n",
    "\n",
    "            if (i==max(all_data.index)):                                          #as above but for the last chunk of tweets\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})\n",
    "                df.to_pickle(filename)\n",
    "                del df\n",
    "                tweet_ids = []\n",
    "                n_grams = [] \n",
    "                print(i)\n",
    "    else:\n",
    "        print(\"Files already exist in this folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_size = 4\n",
      "features/tweet_word_4grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 3\n",
      "features/tweet_word_3grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 2\n",
      "features/tweet_word_2grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 1\n",
      "features/tweet_word_1grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n"
     ]
    }
   ],
   "source": [
    "progs = [500,500,500,500]\n",
    "folders = ['features/tweet_word_4grams/','features/tweet_word_3grams/','features/tweet_word_2grams/','features/tweet_word_1grams/']\n",
    "n_gram_sizes = [4,3,2,1]\n",
    "\n",
    "for prog, folder, n_gram_size in zip(progs, folders, n_gram_sizes):\n",
    "    print('n_gram_size = ' + str(n_gram_size))\n",
    "    create_ngrams(prog, folder, n_gram_size, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      n_gram             tweet_id\n",
      "0         your argu independ  1148665559325794305\n",
      "1       argu independ nicola  1148665559325794305\n",
      "2   independ nicola sturgeon  1148665559325794305\n",
      "3       nicola sturgeon sinc  1148665559325794305\n",
      "4         sturgeon sinc that  1148665559325794305\n",
      "5           sinc that speech  1148665559325794305\n",
      "6          not meet ordinari  1155940342098644993\n",
      "7         meet ordinari scot  1155940342098644993\n",
      "8        ordinari scot visit  1155940342098644993\n",
      "9             scot visit big  1155940342098644993\n",
      "10          visit big fearti  1155940342098644993\n",
      "11       big fearti unfortun  1155940342098644993\n",
      "12       fearti unfortun ive  1155940342098644993\n",
      "13          unfortun ive got  1155940342098644993\n",
      "14             ive got heavi  1155940342098644993\n",
      "15           got heavi timet  1155940342098644993\n",
      "16           heavi timet ive  1155940342098644993\n",
      "17             timet ive got  1155940342098644993\n",
      "18                ive got go  1155940342098644993\n",
      "19               got go meet  1155940342098644993\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_pickle('features/tweet_word_3grams/progress_3500.pickle')\n",
    "print(x.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
