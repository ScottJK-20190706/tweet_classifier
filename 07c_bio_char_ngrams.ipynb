{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and evaluation into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('pickle_files/eval_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Bio'] = all_data['Bio'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].fillna('')\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Bio'] = [p.sub('', x) for x in all_data['Bio'].tolist()] #remove the punctuation\n",
    "for i in all_data.index:\n",
    "    #print(i)\n",
    "    #all_data.loc[i,'Bio'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Bio'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Bio'])\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       here there and everywhere\\n\\nrtw   doesnt nece...\n",
       "1       people may forget what you said they may forge...\n",
       "2                          official account uk today news\n",
       "3       an economist interested in politics technology...\n",
       "4                                                        \n",
       "5                                                     tbc\n",
       "6       cms contentmanagement smallbiz entrepreneur bu...\n",
       "7                                                        \n",
       "8       eu citizen  lets rid the world of the butchers...\n",
       "9       learn to listen then listen to learn\\nstudying...\n",
       "10      old fashioned soul who simply wants the best f...\n",
       "11                                      straight to video\n",
       "12      rangersfc advocate of animal welfare loathe th...\n",
       "13                                  singlecell paramecium\n",
       "14      bringing the news unbiased from scotland for a...\n",
       "15      born under a union jack  if you like random ou...\n",
       "16                                        scottishbritish\n",
       "17      ex rn 24 years now i drive a cab chelsea fan t...\n",
       "18                                                       \n",
       "19      like 3 stripe vw mufc good music dislike liars...\n",
       "20      parkinsons speaker and lejog completer next up...\n",
       "21      retired newspaper editor graduate defense lang...\n",
       "22      made in scotland by an irishman snp supporter ...\n",
       "23      scottish journalist and blogger based in dubli...\n",
       "24                 aging farmboy remembers exciting times\n",
       "25                       breaking pro brexit news stories\n",
       "26      blogger and podcaster  music film television a...\n",
       "27                                                qctardy\n",
       "28                                                   news\n",
       "29        constantly amazed by the stupidity in the world\n",
       "                              ...                        \n",
       "5074     europeanremaininterestshistorygovernment and ...\n",
       "5075                                                     \n",
       "5076                   never play leapfrog with a unicorn\n",
       "5077                                 좆메이징은 못말려  한화 lgm 阪神\n",
       "5078    englishright wingstrong views trump supporterh...\n",
       "5079                                                   20\n",
       "5080    welsh fruitcake who tweets a lot of nonsense a...\n",
       "5081    still working towards retirement love family w...\n",
       "5082    environmentalist traveler and optimist togethe...\n",
       "5083                                                     \n",
       "5084    scottish conservative councillor for forfar ec...\n",
       "5085    bringing the news unbiased from scotland for a...\n",
       "5086    lol im not a politician but i am a right honou...\n",
       "5087         interested in scotch politics floating voter\n",
       "5088                                                     \n",
       "5089                                  auribus teneo lupum\n",
       "5090                                                     \n",
       "5091                                                     \n",
       "5092    prolific paedophile hater against the vile uk ...\n",
       "5093             sorry miss i was giving myself an oiljob\n",
       "5094                                             eclectic\n",
       "5095    cider curry fishing  dexter the mentalist brea...\n",
       "5096    scottish liberal centrist dad do hard sums fca...\n",
       "5097                                                     \n",
       "5098    sport  tv  ba sports journalism  northeast chy...\n",
       "5099                                                     \n",
       "5100    spinal stenosis sufferer unemployed solicitor ...\n",
       "5101    currently obsessed with ai in martech and bran...\n",
       "5102    assistant editor at conservativehome contact h...\n",
       "5103                                                  nil\n",
       "Name: Bio, Length: 5104, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Bio'] = all_data['Bio'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for i in all_data.index:\n",
    "    for w in range(0,len(all_data.Bio[i])):\n",
    "        all_data.Bio[i][w] = ps.stem(all_data.Bio[i][w])\n",
    "\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in all_data.index:\n",
    "    all_data.Bio[i] = ' '.join(all_data.Bio[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding char ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(prog, folder, n_gram_size,file):\n",
    "    \n",
    "    n = n_gram_size\n",
    "    tweet_ids = [] #initialise list of tweet ids\n",
    "    n_grams = [] #initialise list of n_grams\n",
    "\n",
    "\n",
    "\n",
    "    progress_dumps = prog #memory becomes a problem with this code.  Set a value n whereby the processed lists of tweets will be saved\n",
    "                         #to disk every n tweets and memory will be cleared***\n",
    "    location = folder + '*.pickle'\n",
    "    print(folder)\n",
    "    x = glob.glob(folder)\n",
    "    print(len(x))\n",
    "    if len(x)<2:\n",
    "        for i in all_data.index:\n",
    "\n",
    "            tweet_id = all_data['Tweet ID'][i]\n",
    "            all_data.Bio[i] = all_data.Bio[i].replace(\" \", \"_\")            #replace whitespace with underscore\n",
    "            if len(all_data.Bio[i])==0:\n",
    "                n_grams = np.append(n_grams,'zzzz')                                 #if there are no tokens add \"zzzz\" for this tweet\n",
    "                tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            else:\n",
    "                for c in range(0,len(all_data.Bio[i])-n+1):                           #iterate along the length of each token\n",
    "                    n_grams = np.append(n_grams,all_data.Bio[i][c:c+n])               #add n_gram to list\n",
    "                    tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            if (i//progress_dumps==i/progress_dumps):                       #save progress and clear memory\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'          #set folder and filename***\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})                       #create dataframe\n",
    "                df.to_pickle(filename)                                      #save\n",
    "                del df                                                      #clear memory\n",
    "                tweet_ids = []                                              #reinitialise list of tweet ids\n",
    "                n_grams = []                                                #reinitialise list of n_grams\n",
    "                print(i)                                                    #print progress\n",
    "\n",
    "            if (i==max(all_data.index)):                                          #as above but for the last chunk of tweets\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})\n",
    "                df.to_pickle(filename)\n",
    "                del df\n",
    "                tweet_ids = []\n",
    "                n_grams = [] \n",
    "                print(i)\n",
    "    else:\n",
    "        print(\"Files already exist in this folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save char ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_size = 4\n",
      "features/bio_char_4grams/\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 3\n",
      "features/bio_char_3grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 2\n",
      "features/bio_char_2grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 1\n",
      "features/bio_char_1grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n"
     ]
    }
   ],
   "source": [
    "progs = [500,500,500,500]\n",
    "folders = ['features/bio_char_4grams/','features/bio_char_3grams/','features/bio_char_2grams/','features/bio_char_1grams/']\n",
    "n_gram_sizes = [4,3,2,1]\n",
    "\n",
    "for prog, folder, n_gram_size in zip(progs, folders, n_gram_sizes):\n",
    "    print('n_gram_size = ' + str(n_gram_size))\n",
    "    create_ngrams(prog, folder, n_gram_size, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_gram             tweet_id\n",
      "0    spor  1148184591532011526\n",
      "1    port  1148184591532011526\n",
      "2    orts  1148184591532011526\n",
      "3    rtsw  1148184591532011526\n",
      "4    tswr  1148184591532011526\n",
      "5    swri  1148184591532011526\n",
      "6    writ  1148184591532011526\n",
      "7    rit_  1148184591532011526\n",
      "8    it_b  1148184591532011526\n",
      "9    t_br  1148184591532011526\n",
      "10   _bro  1148184591532011526\n",
      "11   broa  1148184591532011526\n",
      "12   road  1148184591532011526\n",
      "13   oadc  1148184591532011526\n",
      "14   adca  1148184591532011526\n",
      "15   dcas  1148184591532011526\n",
      "16   cast  1148184591532011526\n",
      "17   ast_  1148184591532011526\n",
      "18   st_s  1148184591532011526\n",
      "19   t_st  1148184591532011526\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_pickle('features/bio_char_4grams/progress_2500.pickle')\n",
    "print(x.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26491"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
