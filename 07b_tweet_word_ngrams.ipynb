{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and evaluation into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('pickle_files/eval_data_formatted.pickle')\n",
    "eval_data = pd.read_pickle('pickle_files/train_data_formatted.pickle')\n",
    "all_data = train_data.append(eval_data)\n",
    "all_data['Tweet ID'] = all_data['Tweet ID'].astype(str) #change the ID to str to avoid potential issues during aggregation\n",
    "all_data = all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #get stopwords from NLTK\n",
    "keep = ['not'] #Waseem/Hovy did not use \"not\" as a stopword\n",
    "stop = [word for word in stop if word not in keep] #Waseem/Hovy did not use \"not\" as a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].str.lower() #lowercase the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation, usernames, hashtags, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].fillna('')\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_data['Tweet'] = [p.sub('', x) for x in all_data['Tweet'].tolist()] #remove the punctuation\n",
    "for i in all_data.index:\n",
    "    #print(i)\n",
    "    #all_data.loc[i,'Bio'] =re.sub('[^A-Za-z0-9]+',\"\",all_data.loc[i,'Bio'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"@[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"http[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "    all_data.loc[i,'Tweet'] =re.sub(\"#[A-Za-z0-9_/:().]+\",  \"\", all_data.loc[i,'Tweet'])\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       matt is this the same nicola sturgeon thats be...\n",
       "1            you know nothing of nicola sturgeon atkinson\n",
       "2       sturgeon letter to new pm demands essential al...\n",
       "3       scotland had a ref and lost they need permissi...\n",
       "4               nicola sturgeon depressing numpty go away\n",
       "5                      i actually despise nicola sturgeon\n",
       "6       what brexit means for scotland a qampa with fi...\n",
       "7                           leo sayer and nicola sturgeon\n",
       "8                                      so vile eh\\n\\n\\n\\n\n",
       "9       yes but when calling someone far right for act...\n",
       "10      virtue signalling reaches a new high try solvi...\n",
       "11      and for some reason a heavily pregnant nicola ...\n",
       "12      that boot sturgeon really needs fucked in the ...\n",
       "13      the just get in from nicola sturgeon at the en...\n",
       "14      sturgeon we may need to accelerate indyref2 plans\n",
       "15      looking forward to next weeks instalment\\n\\nal...\n",
       "16      if only youd turn your gaze onto what is happe...\n",
       "17      heres a plan all the remainers can go and live...\n",
       "18      read this comment and sign the petition its wr...\n",
       "19      niclola sturgeon really is a tedious annoying ...\n",
       "20      you do wonder if england wanted to leave the u...\n",
       "21      dangerous  government intent on forcing no dea...\n",
       "22      i am voting for the person delivering truth  n...\n",
       "23      nicola sturgeon has the power to stop a nodeal...\n",
       "24      glory hallelujah\\nbest news to end a bad week\\...\n",
       "25      brexit live nicola sturgeon attacked for faili...\n",
       "26      the brexit referendum has caused chaos\\n\\nthe ...\n",
       "27                                   art nicola sturgeon \n",
       "28      id like to drop the toilet seat on sturgeons h...\n",
       "29         so prince george is related to nicola sturgeon\n",
       "                              ...                        \n",
       "5074                                      nicola sturgeon\n",
       "5075    sorry not nicola sturgeon but the snp which sh...\n",
       "5076    nicola sturgeon in an independent scotlandbett...\n",
       "5077                                                 아 ㅋㅋ\n",
       "5078    why is nobody sturgeon there anyway at least c...\n",
       "5079                                    fuck off sturgeon\n",
       "5080                                      nicola sturgeon\n",
       "5081    is it not high handed and arrogant of ms sturg...\n",
       "5082    nicola sturgeon slams uk government for plan t...\n",
       "5083    o look scotland is part of the untied kingdom ...\n",
       "5084    nicola sturgeon became first minister of scotl...\n",
       "5085    there must be parity in men and womens scottis...\n",
       "5086             the snp and sturgeon do not speak for me\n",
       "5087                    peter murrell and nicola sturgeon\n",
       "5088    read this comment and sign the petition spain ...\n",
       "5089               nicola sturgeon just needs to fuck off\n",
       "5090    is it so wrong i want to slap nicola sturgeon ...\n",
       "5091    brilliant  new found respect for nicola sturgeon \n",
       "5092    nicola sturgeons 6m baby box is a pointless gi...\n",
       "5093    basically scotland will always have its hand o...\n",
       "5094    sturgeon just keeps picking pointless fights t...\n",
       "5095                       tony blair and nicola sturgeon\n",
       "5096    admirable but at the same time sturgeon has al...\n",
       "5097    nicola sturgeon needs shagging with barbed wir...\n",
       "5098                           god i hate nicola sturgeon\n",
       "5099    you really fancy your bara   nearly as bad as ...\n",
       "5100    good have her arrested for treason or hate cri...\n",
       "5101    great thread\\nsturgeonlucas already work well ...\n",
       "5102    an english nationalist conservative party woul...\n",
       "5103    well see it accompanies sturgeons preparations...\n",
       "Name: Tweet, Length: 5104, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "all_data['Tweet'] = all_data['Tweet'].apply(word_tokenize) #tokenize the text\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for i in all_data.index:\n",
    "    for w in range(0,len(all_data.Tweet[i])):\n",
    "        all_data.Tweet[i][w] = ps.stem(all_data.Tweet[i][w])\n",
    "\n",
    "print(len(np.unique(all_data['Tweet ID']))) #for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for finding word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(prog, folder, n_gram_size,file):\n",
    "\n",
    "    tweet_ids = [] #initialise list of tweet ids\n",
    "    n_grams = [] #initialise list of n_grams\n",
    "\n",
    "\n",
    "\n",
    "    progress_dumps = prog #memory becomes a problem with this code.  Set a value n whereby the processed lists of tweets will be saved\n",
    "                         #to disk every n tweets and memory will be cleared***\n",
    "    location = folder + '*.pickle'\n",
    "    print(folder)\n",
    "    x = glob.glob(folder)\n",
    "    print(len(x))\n",
    "    if len(x)<2:\n",
    "        for i in all_data.index:\n",
    "\n",
    "            tweet_id = all_data['Tweet ID'][i]                                  #get the tweet id\n",
    "            tokens = all_data.Tweet[i]                                          #get the associated tokens\n",
    "            n=n_gram_size                                                   #set the n_gram length***\n",
    "            if len(all_data.Tweet[i])==0:\n",
    "                n_grams = np.append(n_grams,'zzzz')                                 #if there are no tokens add \"zzzz\" for this tweet\n",
    "                tweet_ids = np.append(tweet_ids,tweet_id)\n",
    "            else:\n",
    "                for c in range(0,len(all_data.Tweet[i])-n+1):\n",
    "                    n_gram = ' '.join(all_data.Tweet[i][c:c+n])\n",
    "                    n_grams = np.append(n_grams,n_gram)               #add n_gram to list\n",
    "                    tweet_ids = np.append(tweet_ids,tweet_id) \n",
    "\n",
    "            if (i//progress_dumps==i/progress_dumps):                       #save progress and clear memory\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'          #set folder and filename***\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})                       #create dataframe\n",
    "                df.to_pickle(filename)                                      #save\n",
    "                del df                                                      #clear memory\n",
    "                tweet_ids = []                                              #reinitialise list of tweet ids\n",
    "                n_grams = []                                                #reinitialise list of n_grams\n",
    "                print(i)                                                    #print progress\n",
    "\n",
    "            if (i==max(all_data.index)):                                          #as above but for the last chunk of tweets\n",
    "                filename = folder + 'progress_' + str(i)+'.pickle'\n",
    "                df = pd.DataFrame({'tweet_id':tweet_ids,\n",
    "                                   'n_gram':n_grams})\n",
    "                df.to_pickle(filename)\n",
    "                del df\n",
    "                tweet_ids = []\n",
    "                n_grams = [] \n",
    "                print(i)\n",
    "    else:\n",
    "        print(\"Files already exist in this folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_size = 4\n",
      "features/tweet_word_4grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 3\n",
      "features/tweet_word_3grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 2\n",
      "features/tweet_word_2grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n",
      "n_gram_size = 1\n",
      "features/tweet_word_1grams/\n",
      "1\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5103\n"
     ]
    }
   ],
   "source": [
    "progs = [500,500,500,500]\n",
    "folders = ['features/tweet_word_4grams/','features/tweet_word_3grams/','features/tweet_word_2grams/','features/tweet_word_1grams/']\n",
    "n_gram_sizes = [4,3,2,1]\n",
    "\n",
    "for prog, folder, n_gram_size in zip(progs, folders, n_gram_sizes):\n",
    "    print('n_gram_size = ' + str(n_gram_size))\n",
    "    create_ngrams(prog, folder, n_gram_size, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      n_gram             tweet_id\n",
      "0         your argu independ  1148665559325794305\n",
      "1       argu independ nicola  1148665559325794305\n",
      "2   independ nicola sturgeon  1148665559325794305\n",
      "3       nicola sturgeon sinc  1148665559325794305\n",
      "4         sturgeon sinc that  1148665559325794305\n",
      "5           sinc that speech  1148665559325794305\n",
      "6          not meet ordinari  1155940342098644993\n",
      "7         meet ordinari scot  1155940342098644993\n",
      "8        ordinari scot visit  1155940342098644993\n",
      "9             scot visit big  1155940342098644993\n",
      "10          visit big fearti  1155940342098644993\n",
      "11       big fearti unfortun  1155940342098644993\n",
      "12       fearti unfortun ive  1155940342098644993\n",
      "13          unfortun ive got  1155940342098644993\n",
      "14             ive got heavi  1155940342098644993\n",
      "15           got heavi timet  1155940342098644993\n",
      "16           heavi timet ive  1155940342098644993\n",
      "17             timet ive got  1155940342098644993\n",
      "18                ive got go  1155940342098644993\n",
      "19               got go meet  1155940342098644993\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_pickle('features/tweet_word_3grams/progress_3500.pickle')\n",
    "print(x.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
