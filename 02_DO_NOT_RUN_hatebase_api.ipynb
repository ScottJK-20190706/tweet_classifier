{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hatebase api key and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hatebase/Hatebase-API-Docs/blob/master/current/v4-2/get_vocabulary.md\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "api_key = 'KAKHM8ADQdCKWNPjAFDGRYUXFNNzEVXW'  #api key for hatebase\n",
    "api_url = 'https://api.hatebase.org/4-2/authenticate'  #authentication endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with hatebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'api_key': api_key}\n",
    "response = requests.post(url=api_url, data=data)  #call the api to authenticate and receive a token\n",
    "resp = response.json()\n",
    "token = resp['result']['token'] #the token for further queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Hatebase api to find the number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.hatebase.org/4-2/get_vocabulary' #endpoint for hate terms\n",
    "data = {'token': token, 'language':'ENG'}\n",
    "response = requests.post(url=api_url, data=data)\n",
    "resp = response.json()\n",
    "pages = resp['number_of_pages'] #count the number of pages of hate terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through the results to create dataframe of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [] #empty series\n",
    "ave_offense = [] #empty series\n",
    "for p in range(1,resp['number_of_pages']+1):\n",
    "    data = {'token': token, 'language':'ENG', 'page':p}\n",
    "    response = requests.post(url=api_url, data=data) #call each page\n",
    "    resp = response.json()\n",
    "    for t in range(0,len(resp['result'])): #get each term on a page\n",
    "        terms = np.append(terms,resp['result'][t]['term']) #append to the series of terms\n",
    "        ave_offense = np.append(ave_offense,resp['result'][t]['average_offensiveness']) #append to the series of terms\n",
    "hate_terms = pd.DataFrame({'hate_terms':terms,\n",
    "                          'ave_offense':ave_offense}) #create dataframe of terms\n",
    "hate_terms.to_pickle('pickle_files/hate_terms.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_terms.to_csv('csv_excel/hate_terms2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
